| distributed init (rank 0, world 1): env://
Loading VIT
2023-09-12 20:50:11,471 [INFO]
=====  Running Parameters    =====
2023-09-12 20:50:11,472 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 2,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 3e-05,
    "iters_per_epoch": 5,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 100,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "/output/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "video_text_pretrain",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 5,
    "weight_decay": 0.05,
    "world_size": 1
}
2023-09-12 20:50:11,472 [INFO]
======  Dataset Attributes  ======
2023-09-12 20:50:11,472 [INFO]
======== bdd_instruct =======
2023-09-12 20:50:11,473 [INFO] {
    "build_info": {
        "train": {
            "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
            "videos_dir": "/root/BDD-X/"
        },
        "val": {
            "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
            "videos_dir": "/root/BDD-X/"
        }
    },
    "data_type": "video",
    "model_type": "llama_v2",
    "num_video_query_token": 32,
    "text_processor": {
        "train": {
            "name": "blip_caption"
        },
        "val": {
            "name": "blip_caption"
        }
    },
    "tokenizer_name": "/root/ckpt/llama-2-7b-chat-hf",
    "vis_processor": {
        "train": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        },
        "val": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        }
    }
}
2023-09-12 20:50:11,473 [INFO]
======  Model Attributes  ======
2023-09-12 20:50:11,473 [INFO] {
    "arch": "video_llama",
    "ckpt": "/root/ckpt/VL_LLaMA_2_7B_Finetuned.pth",
    "drop_path_rate": 0,
    "equip_audio_branch": false,
    "freeze_qformer": true,
    "freeze_vit": true,
    "frozen_audio_Qformer": true,
    "frozen_llama_proj": false,
    "frozen_video_Qformer": false,
    "fusion_head_layers": 2,
    "fusion_header_type": "seqTransf",
    "image_size": 224,
    "llama_model": "/root/ckpt/llama-2-7b-chat-hf",
    "max_frame_pos": 32,
    "max_txt_len": 320,
    "model_type": "pretrain_llama_v2",
    "num_query_token": 32,
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
2023-09-12 20:50:11,473 [INFO] Building datasets...
Loading VIT Done
Loading Q-Former
2023-09-12 20:50:47,993 [INFO] freeze vision encoder
2023-09-12 20:50:50,794 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth
2023-09-12 20:50:50,807 [INFO] freeze Qformer
2023-09-12 20:50:50,807 [INFO] Loading Q-Former Done
2023-09-12 20:50:50,807 [INFO] Loading LLAMA Tokenizer
Using pad_token, but it is not set yet.
2023-09-12 20:50:50,960 [INFO] Loading LLAMA Model

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.63s/it]
2023-09-12 20:51:56,891 [INFO] Loading LLAMA Done
2023-09-12 20:51:56,891 [INFO] Loading LLAMA proj
2023-09-12 20:51:56,908 [INFO] LLAMA proj is not frozen
2023-09-12 20:51:56,909 [INFO] Loading llama_proj Done
Load first Checkpoint: /root/ckpt/VL_LLaMA_2_7B_Finetuned.pth
2023-09-12 20:51:57,928 [INFO] video_Qformer is not frozen
2023-09-12 20:51:58,096 [INFO] Start training
2023-09-12 20:52:02,421 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-09-12 20:52:02,421 [INFO] Loaded 10 records for train split from the dataset.
2023-09-12 20:52:02,421 [INFO] Loaded 10 records for val split from the dataset.
2023-09-12 20:52:02,436 [INFO] number of trainable parameters: 22104064
2023-09-12 20:52:02,437 [INFO] Start training epoch 0, 5 iters per inner epoch.
dict_keys(['train', 'val'])
module.video_query_tokens
module.llama_proj.weight
module.llama_proj.bias
module.video_frame_position_embedding.weight
module.video_Qformer.bert.embeddings.LayerNorm.weight
module.video_Qformer.bert.embeddings.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
2023-09-12 20:52:05,018 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [0/5]  eta: 0:00:12  lr: 0.000001  loss: 2.2720  time: 2.5740  data: 0.0000  max mem: 33000
2023-09-12 20:52:06,759 [INFO] Averaged stats: lr: 0.0000  loss: 2.0897
2023-09-12 20:52:06,762 [INFO] Evaluating on val.
2023-09-12 20:52:06,774 [INFO] Start training
2023-09-12 20:52:06,792 [INFO] Start training epoch 1, 5 iters per inner epoch.
Train: data epoch: [0]  [4/5]  eta: 0:00:00  lr: 0.000024  loss: 1.9258  time: 0.8641  data: 0.0000  max mem: 35723
Train: data epoch: [0] Total time: 0:00:04 (0.8644 s / it)
JBE
Train: data epoch: [1]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.9663  time: 3.1852  data: 0.0000  max mem: 35723
Train: data epoch: [1]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.9333  time: 0.9151  data: 0.0000  max mem: 35723
Train: data epoch: [1] Total time: 0:00:04 (0.9154 s / it)
JBE
2023-09-12 20:52:11,369 [INFO] Averaged stats: lr: 0.0000  loss: 1.8879
2023-09-12 20:52:11,372 [INFO] Evaluating on val.
2023-09-12 20:52:11,383 [INFO] Start training
2023-09-12 20:52:11,402 [INFO] Start training epoch 2, 5 iters per inner epoch.
Train: data epoch: [2]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.7643  time: 3.2076  data: 0.0000  max mem: 35723
Train: data epoch: [2]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.8292  time: 0.9293  data: 0.0000  max mem: 35723
Train: data epoch: [2] Total time: 0:00:04 (0.9296 s / it)
JBE
2023-09-12 20:52:16,051 [INFO] Averaged stats: lr: 0.0000  loss: 1.7928
2023-09-12 20:52:16,053 [INFO] Evaluating on val.
2023-09-12 20:52:16,065 [INFO] Start training
2023-09-12 20:52:16,084 [INFO] Start training epoch 3, 5 iters per inner epoch.
Train: data epoch: [3]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.6741  time: 3.1476  data: 0.0000  max mem: 35723
2023-09-12 20:52:20,659 [INFO] Averaged stats: lr: 0.0000  loss: 1.7303
2023-09-12 20:52:20,662 [INFO] Evaluating on val.
2023-09-12 20:52:20,674 [INFO] Start training
2023-09-12 20:52:20,693 [INFO] Start training epoch 4, 5 iters per inner epoch.
Train: data epoch: [3]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.6887  time: 0.9147  data: 0.0000  max mem: 35723
Train: data epoch: [3] Total time: 0:00:04 (0.9150 s / it)
JBE
Train: data epoch: [4]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.5905  time: 3.1645  data: 0.0000  max mem: 35723
Train: data epoch: [4]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.7931  time: 0.9118  data: 0.0000  max mem: 35723
Train: data epoch: [4] Total time: 0:00:04 (0.9121 s / it)
JBE
2023-09-12 20:52:25,254 [INFO] Averaged stats: lr: 0.0000  loss: 1.6811
2023-09-12 20:52:25,257 [INFO] Evaluating on val.
2023-09-12 20:52:25,269 [INFO] Start training
2023-09-12 20:52:25,295 [INFO] Start training epoch 5, 5 iters per inner epoch.
Train: data epoch: [5]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.6205  time: 3.0820  data: 0.0000  max mem: 35723
Train: data epoch: [5]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.5089  time: 0.9066  data: 0.0000  max mem: 35723
Train: data epoch: [5] Total time: 0:00:04 (0.9070 s / it)
JBE
2023-09-12 20:52:29,831 [INFO] Averaged stats: lr: 0.0000  loss: 1.6230
2023-09-12 20:52:29,834 [INFO] Evaluating on val.
2023-09-12 20:52:29,846 [INFO] Start training
2023-09-12 20:52:29,864 [INFO] Start training epoch 6, 5 iters per inner epoch.
Train: data epoch: [6]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.5200  time: 3.1861  data: 0.0000  max mem: 35723
2023-09-12 20:52:34,504 [INFO] Averaged stats: lr: 0.0000  loss: 1.5721
2023-09-12 20:52:34,506 [INFO] Evaluating on val.
2023-09-12 20:52:34,519 [INFO] Start training
2023-09-12 20:52:34,538 [INFO] Start training epoch 7, 5 iters per inner epoch.
Train: data epoch: [6]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.4715  time: 0.9276  data: 0.0000  max mem: 35723
Train: data epoch: [6] Total time: 0:00:04 (0.9278 s / it)
JBE
Train: data epoch: [7]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.5845  time: 3.1927  data: 0.0000  max mem: 35723
2023-09-12 20:52:39,092 [INFO] Averaged stats: lr: 0.0000  loss: 1.5338
2023-09-12 20:52:39,094 [INFO] Evaluating on val.
2023-09-12 20:52:39,106 [INFO] Start training
2023-09-12 20:52:39,125 [INFO] Start training epoch 8, 5 iters per inner epoch.
Train: data epoch: [7]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.4863  time: 0.9103  data: 0.0000  max mem: 35723
Train: data epoch: [7] Total time: 0:00:04 (0.9106 s / it)
JBE
Train: data epoch: [8]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.6832  time: 3.2537  data: 0.0000  max mem: 35723
Train: data epoch: [8]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.4265  time: 0.9362  data: 0.0000  max mem: 35723
Train: data epoch: [8] Total time: 0:00:04 (0.9365 s / it)
JBE
2023-09-12 20:52:43,808 [INFO] Averaged stats: lr: 0.0000  loss: 1.4794
2023-09-12 20:52:43,810 [INFO] Evaluating on val.
2023-09-12 20:52:43,822 [INFO] Start training
2023-09-12 20:52:43,841 [INFO] Start training epoch 9, 5 iters per inner epoch.
Train: data epoch: [9]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.5141  time: 3.3502  data: 0.0000  max mem: 35723
2023-09-12 20:52:48,616 [INFO] Averaged stats: lr: 0.0000  loss: 1.4311
2023-09-12 20:52:48,618 [INFO] Evaluating on val.
2023-09-12 20:52:48,630 [INFO] Start training
2023-09-12 20:52:48,649 [INFO] Start training epoch 10, 5 iters per inner epoch.
Train: data epoch: [9]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.3966  time: 0.9546  data: 0.0000  max mem: 35723
Train: data epoch: [9] Total time: 0:00:04 (0.9549 s / it)
JBE
Train: data epoch: [10]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.1973  time: 3.0664  data: 0.0000  max mem: 35723
2023-09-12 20:52:53,203 [INFO] Averaged stats: lr: 0.0000  loss: 1.3755
2023-09-12 20:52:53,205 [INFO] Evaluating on val.
2023-09-12 20:52:53,217 [INFO] Start training
2023-09-12 20:52:53,236 [INFO] Start training epoch 11, 5 iters per inner epoch.
Train: data epoch: [10]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 1.3875  time: 0.9105  data: 0.0000  max mem: 35723
Train: data epoch: [10] Total time: 0:00:04 (0.9108 s / it)
JBE
Train: data epoch: [11]  [0/5]  eta: 0:00:16  lr: 0.000029  loss: 1.3089  time: 3.2005  data: 0.0000  max mem: 35723
Train: data epoch: [11]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 1.1777  time: 0.9313  data: 0.0000  max mem: 35723
Train: data epoch: [11] Total time: 0:00:04 (0.9316 s / it)
JBE
2023-09-12 20:52:57,895 [INFO] Averaged stats: lr: 0.0000  loss: 1.3372
2023-09-12 20:52:57,897 [INFO] Evaluating on val.
2023-09-12 20:52:57,909 [INFO] Start training
2023-09-12 20:52:57,927 [INFO] Start training epoch 12, 5 iters per inner epoch.
Train: data epoch: [12]  [0/5]  eta: 0:00:16  lr: 0.000029  loss: 1.5050  time: 3.2136  data: 0.0000  max mem: 35723
2023-09-12 20:53:02,575 [INFO] Averaged stats: lr: 0.0000  loss: 1.2676
2023-09-12 20:53:02,577 [INFO] Evaluating on val.
2023-09-12 20:53:02,589 [INFO] Start training
2023-09-12 20:53:02,608 [INFO] Start training epoch 13, 5 iters per inner epoch.
Train: data epoch: [12]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 0.9964  time: 0.9292  data: 0.0000  max mem: 35723
Train: data epoch: [12] Total time: 0:00:04 (0.9295 s / it)
JBE
Train: data epoch: [13]  [0/5]  eta: 0:00:16  lr: 0.000029  loss: 1.3553  time: 3.2634  data: 0.0000  max mem: 35723
Train: data epoch: [13]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 1.1595  time: 0.9723  data: 0.0000  max mem: 35723
Train: data epoch: [13] Total time: 0:00:04 (0.9727 s / it)
JBE
2023-09-12 20:53:07,472 [INFO] Averaged stats: lr: 0.0000  loss: 1.2389
2023-09-12 20:53:07,474 [INFO] Evaluating on val.
2023-09-12 20:53:07,486 [INFO] Start training
2023-09-12 20:53:07,506 [INFO] Start training epoch 14, 5 iters per inner epoch.
Train: data epoch: [14]  [0/5]  eta: 0:00:15  lr: 0.000029  loss: 1.2391  time: 3.1711  data: 0.0000  max mem: 35723
Train: data epoch: [14]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 1.2309  time: 0.9370  data: 0.0000  max mem: 35723
Train: data epoch: [14] Total time: 0:00:04 (0.9373 s / it)
JBE
2023-09-12 20:53:12,193 [INFO] Averaged stats: lr: 0.0000  loss: 1.1954
2023-09-12 20:53:12,196 [INFO] Evaluating on val.
2023-09-12 20:53:12,208 [INFO] Start training
2023-09-12 20:53:12,227 [INFO] Start training epoch 15, 5 iters per inner epoch.
Train: data epoch: [15]  [0/5]  eta: 0:00:15  lr: 0.000029  loss: 1.0926  time: 3.1563  data: 0.0000  max mem: 35723
2023-09-12 20:53:16,814 [INFO] Averaged stats: lr: 0.0000  loss: 1.1332
2023-09-12 20:53:16,817 [INFO] Evaluating on val.
2023-09-12 20:53:16,829 [INFO] Start training
2023-09-12 20:53:16,852 [INFO] Start training epoch 16, 5 iters per inner epoch.
Train: data epoch: [15]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 1.3156  time: 0.9170  data: 0.0000  max mem: 35723
Train: data epoch: [15] Total time: 0:00:04 (0.9173 s / it)
JBE
Train: data epoch: [16]  [0/5]  eta: 0:00:16  lr: 0.000029  loss: 1.1306  time: 3.2213  data: 0.0000  max mem: 35723
Train: data epoch: [16]  [4/5]  eta: 0:00:00  lr: 0.000029  loss: 0.9414  time: 0.9420  data: 0.0000  max mem: 35723
Train: data epoch: [16] Total time: 0:00:04 (0.9424 s / it)
JBE
2023-09-12 20:53:21,565 [INFO] Averaged stats: lr: 0.0000  loss: 1.0679
2023-09-12 20:53:21,568 [INFO] Evaluating on val.
2023-09-12 20:53:21,584 [INFO] Start training
2023-09-12 20:53:21,602 [INFO] Start training epoch 17, 5 iters per inner epoch.
Train: data epoch: [17]  [0/5]  eta: 0:00:15  lr: 0.000029  loss: 0.9317  time: 3.1608  data: 0.0000  max mem: 35723
Train: data epoch: [17]  [4/5]  eta: 0:00:00  lr: 0.000028  loss: 1.0582  time: 0.9228  data: 0.0000  max mem: 35723
Train: data epoch: [17] Total time: 0:00:04 (0.9231 s / it)
JBE
2023-09-12 20:53:26,218 [INFO] Averaged stats: lr: 0.0000  loss: 1.0267
2023-09-12 20:53:26,220 [INFO] Evaluating on val.
2023-09-12 20:53:26,233 [INFO] Start training
2023-09-12 20:53:26,251 [INFO] Start training epoch 18, 5 iters per inner epoch.
Train: data epoch: [18]  [0/5]  eta: 0:00:15  lr: 0.000028  loss: 1.1089  time: 3.1509  data: 0.0000  max mem: 35723
2023-09-12 20:53:30,843 [INFO] Averaged stats: lr: 0.0000  loss: 0.9910
2023-09-12 20:53:30,845 [INFO] Evaluating on val.
2023-09-12 20:53:30,857 [INFO] Start training
2023-09-12 20:53:30,875 [INFO] Start training epoch 19, 5 iters per inner epoch.
Train: data epoch: [18]  [4/5]  eta: 0:00:00  lr: 0.000028  loss: 0.8796  time: 0.9178  data: 0.0000  max mem: 35723
Train: data epoch: [18] Total time: 0:00:04 (0.9182 s / it)
JBE
Train: data epoch: [19]  [0/5]  eta: 0:00:16  lr: 0.000028  loss: 1.1055  time: 3.3244  data: 0.0000  max mem: 35723
Train: data epoch: [19]  [4/5]  eta: 0:00:00  lr: 0.000028  loss: 0.9512  time: 0.9465  data: 0.0000  max mem: 35723
Train: data epoch: [19] Total time: 0:00:04 (0.9469 s / it)
JBE
2023-09-12 20:53:35,611 [INFO] Averaged stats: lr: 0.0000  loss: 0.9522
2023-09-12 20:53:35,613 [INFO] Evaluating on val.
2023-09-12 20:53:35,625 [INFO] Start training
2023-09-12 20:53:35,644 [INFO] Start training epoch 20, 5 iters per inner epoch.
Train: data epoch: [20]  [0/5]  eta: 0:00:16  lr: 0.000028  loss: 0.9777  time: 3.2290  data: 0.0000  max mem: 35723
Train: data epoch: [20]  [4/5]  eta: 0:00:00  lr: 0.000028  loss: 0.9599  time: 0.9249  data: 0.0000  max mem: 35723
Train: data epoch: [20] Total time: 0:00:04 (0.9252 s / it)
JBE
2023-09-12 20:53:40,271 [INFO] Averaged stats: lr: 0.0000  loss: 0.9035
2023-09-12 20:53:40,273 [INFO] Evaluating on val.
2023-09-12 20:53:40,285 [INFO] Start training
2023-09-12 20:53:40,303 [INFO] Start training epoch 21, 5 iters per inner epoch.
Train: data epoch: [21]  [0/5]  eta: 0:00:15  lr: 0.000028  loss: 0.7325  time: 3.1558  data: 0.0000  max mem: 35723
2023-09-12 20:53:44,940 [INFO] Averaged stats: lr: 0.0000  loss: 0.8937
2023-09-12 20:53:44,942 [INFO] Evaluating on val.
2023-09-12 20:53:44,954 [INFO] Start training
2023-09-12 20:53:44,972 [INFO] Start training epoch 22, 5 iters per inner epoch.
Train: data epoch: [21]  [4/5]  eta: 0:00:00  lr: 0.000028  loss: 0.9076  time: 0.9268  data: 0.0000  max mem: 35723
Train: data epoch: [21] Total time: 0:00:04 (0.9271 s / it)
JBE
Train: data epoch: [22]  [0/5]  eta: 0:00:16  lr: 0.000028  loss: 0.9125  time: 3.2251  data: 0.0000  max mem: 35723
Train: data epoch: [22]  [4/5]  eta: 0:00:00  lr: 0.000028  loss: 1.0478  time: 0.9255  data: 0.0000  max mem: 35723
Train: data epoch: [22] Total time: 0:00:04 (0.9258 s / it)
JBE
2023-09-12 20:53:49,602 [INFO] Averaged stats: lr: 0.0000  loss: 0.8258
2023-09-12 20:53:49,605 [INFO] Evaluating on val.
2023-09-12 20:53:49,617 [INFO] Start training
2023-09-12 20:53:49,636 [INFO] Start training epoch 23, 5 iters per inner epoch.
Train: data epoch: [23]  [0/5]  eta: 0:00:15  lr: 0.000028  loss: 0.7865  time: 3.1903  data: 0.0000  max mem: 35723
Train: data epoch: [23]  [4/5]  eta: 0:00:00  lr: 0.000027  loss: 0.6850  time: 0.9313  data: 0.0000  max mem: 35723
Train: data epoch: [23] Total time: 0:00:04 (0.9316 s / it)
JBE
2023-09-12 20:53:54,295 [INFO] Averaged stats: lr: 0.0000  loss: 0.7921
2023-09-12 20:53:54,297 [INFO] Evaluating on val.
2023-09-12 20:53:54,310 [INFO] Start training
2023-09-12 20:53:54,329 [INFO] Start training epoch 24, 5 iters per inner epoch.
Train: data epoch: [24]  [0/5]  eta: 0:00:15  lr: 0.000027  loss: 0.7112  time: 3.1720  data: 0.0000  max mem: 35723
2023-09-12 20:53:58,930 [INFO] Averaged stats: lr: 0.0000  loss: 0.7657
2023-09-12 20:53:58,933 [INFO] Evaluating on val.
2023-09-12 20:53:58,945 [INFO] Start training
2023-09-12 20:53:58,964 [INFO] Start training epoch 25, 5 iters per inner epoch.
Train: data epoch: [24]  [4/5]  eta: 0:00:00  lr: 0.000027  loss: 0.7360  time: 0.9198  data: 0.0000  max mem: 35723
Train: data epoch: [24] Total time: 0:00:04 (0.9202 s / it)
JBE
Train: data epoch: [25]  [0/5]  eta: 0:00:16  lr: 0.000027  loss: 0.8148  time: 3.3054  data: 0.0000  max mem: 35723
Train: data epoch: [25]  [4/5]  eta: 0:00:00  lr: 0.000027  loss: 0.5124  time: 0.9510  data: 0.0000  max mem: 35723
Train: data epoch: [25] Total time: 0:00:04 (0.9514 s / it)
JBE
2023-09-12 20:54:03,721 [INFO] Averaged stats: lr: 0.0000  loss: 0.6895
2023-09-12 20:54:03,724 [INFO] Evaluating on val.
2023-09-12 20:54:03,737 [INFO] Start training
2023-09-12 20:54:03,759 [INFO] Start training epoch 26, 5 iters per inner epoch.
Train: data epoch: [26]  [0/5]  eta: 0:00:16  lr: 0.000027  loss: 0.6507  time: 3.2408  data: 0.0000  max mem: 35723
Train: data epoch: [26]  [4/5]  eta: 0:00:00  lr: 0.000027  loss: 0.7429  time: 0.9384  data: 0.0000  max mem: 35723
Train: data epoch: [26] Total time: 0:00:04 (0.9388 s / it)
2023-09-12 20:54:08,454 [INFO] Averaged stats: lr: 0.0000  loss: 0.6466
2023-09-12 20:54:08,458 [INFO] Evaluating on val.
2023-09-12 20:54:08,473 [INFO] Start training
2023-09-12 20:54:08,492 [INFO] Start training epoch 27, 5 iters per inner epoch.
JBE
Train: data epoch: [27]  [0/5]  eta: 0:00:16  lr: 0.000027  loss: 0.6017  time: 3.2498  data: 0.0000  max mem: 35723
2023-09-12 20:54:13,239 [INFO] Averaged stats: lr: 0.0000  loss: 0.6439
2023-09-12 20:54:13,243 [INFO] Evaluating on val.
2023-09-12 20:54:13,256 [INFO] Start training
2023-09-12 20:54:13,274 [INFO] Start training epoch 28, 5 iters per inner epoch.
Train: data epoch: [27]  [4/5]  eta: 0:00:00  lr: 0.000026  loss: 0.5794  time: 0.9489  data: 0.0000  max mem: 35723
Train: data epoch: [27] Total time: 0:00:04 (0.9492 s / it)
JBE
Train: data epoch: [28]  [0/5]  eta: 0:00:16  lr: 0.000026  loss: 0.4260  time: 3.2645  data: 0.0000  max mem: 35723
Train: data epoch: [28]  [4/5]  eta: 0:00:00  lr: 0.000026  loss: 0.6418  time: 0.9569  data: 0.0000  max mem: 35723
Train: data epoch: [28] Total time: 0:00:04 (0.9573 s / it)
JBE
2023-09-12 20:54:18,062 [INFO] Averaged stats: lr: 0.0000  loss: 0.5883
2023-09-12 20:54:18,066 [INFO] Evaluating on val.
2023-09-12 20:54:18,080 [INFO] Start training
2023-09-12 20:54:18,098 [INFO] Start training epoch 29, 5 iters per inner epoch.
Train: data epoch: [29]  [0/5]  eta: 0:00:16  lr: 0.000026  loss: 0.6991  time: 3.2007  data: 0.0000  max mem: 35723
2023-09-12 20:54:23,097 [INFO] Averaged stats: lr: 0.0000  loss: 0.5588
2023-09-12 20:54:23,101 [INFO] Evaluating on val.
2023-09-12 20:54:23,113 [INFO] Start training
2023-09-12 20:54:23,131 [INFO] Start training epoch 30, 5 iters per inner epoch.
Train: data epoch: [29]  [4/5]  eta: 0:00:00  lr: 0.000026  loss: 0.5005  time: 0.9994  data: 0.0000  max mem: 35723
Train: data epoch: [29] Total time: 0:00:04 (0.9998 s / it)
JBE
Train: data epoch: [30]  [0/5]  eta: 0:00:15  lr: 0.000026  loss: 0.5033  time: 3.1879  data: 0.0000  max mem: 35723
Train: data epoch: [30]  [4/5]  eta: 0:00:00  lr: 0.000026  loss: 0.6242  time: 0.9207  data: 0.0000  max mem: 35723
Train: data epoch: [30] Total time: 0:00:04 (0.9210 s / it)
JBE
2023-09-12 20:54:27,737 [INFO] Averaged stats: lr: 0.0000  loss: 0.4885
2023-09-12 20:54:27,740 [INFO] Evaluating on val.
2023-09-12 20:54:27,751 [INFO] Start training
2023-09-12 20:54:27,774 [INFO] Start training epoch 31, 5 iters per inner epoch.
Train: data epoch: [31]  [0/5]  eta: 0:00:16  lr: 0.000026  loss: 0.4814  time: 3.2048  data: 0.0000  max mem: 35723
Train: data epoch: [31]  [4/5]  eta: 0:00:00  lr: 0.000025  loss: 0.5364  time: 0.9273  data: 0.0000  max mem: 35723
Train: data epoch: [31] Total time: 0:00:04 (0.9276 s / it)
JBE
2023-09-12 20:54:32,413 [INFO] Averaged stats: lr: 0.0000  loss: 0.4741
2023-09-12 20:54:32,415 [INFO] Evaluating on val.
2023-09-12 20:54:32,428 [INFO] Start training
2023-09-12 20:54:32,446 [INFO] Start training epoch 32, 5 iters per inner epoch.
Train: data epoch: [32]  [0/5]  eta: 0:00:15  lr: 0.000025  loss: 0.6031  time: 3.1971  data: 0.0000  max mem: 35723
2023-09-12 20:54:37,173 [INFO] Averaged stats: lr: 0.0000  loss: 0.4584
2023-09-12 20:54:37,176 [INFO] Evaluating on val.
2023-09-12 20:54:37,191 [INFO] Start training
2023-09-12 20:54:37,211 [INFO] Start training epoch 33, 5 iters per inner epoch.
Train: data epoch: [32]  [4/5]  eta: 0:00:00  lr: 0.000025  loss: 0.3163  time: 0.9449  data: 0.0000  max mem: 35723
Train: data epoch: [32] Total time: 0:00:04 (0.9453 s / it)
JBE
Error in sys.excepthook:
Traceback (most recent call last):
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/lib/exit_hooks.py", line 41, in exc_handler
    def exc_handler(
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/video_llama/datasets/datasets/dataloader_utils.py", line 147, in __next__
    data = next(self.iter_loader)
StopIteration
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/train.py", line 114, in <module>
    main()
  File "/root/vision-assistant-for-driving/train.py", line 110, in main
    runner.train(wandb)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 379, in train
    train_stats = self.train_epoch(cur_epoch)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 445, in train_epoch
    return self.task.train_epoch(
  File "/root/vision-assistant-for-driving/video_llama/tasks/base_task.py", line 124, in train_epoch
    return self._train_inner_loop(
  File "/root/vision-assistant-for-driving/video_llama/tasks/base_task.py", line 215, in _train_inner_loop
    samples = next(data_loader)
  File "/root/vision-assistant-for-driving/video_llama/datasets/datasets/dataloader_utils.py", line 43, in __next__
    return next(self.loaders[loader_idx])
  File "/root/vision-assistant-for-driving/video_llama/datasets/datasets/dataloader_utils.py", line 154, in __next__
    data = next(self.iter_loader)
  File "/root/vision-assistant-for-driving/video_llama/datasets/datasets/dataloader_utils.py", line 60, in __iter__
    self.preload(loader_it)
  File "/root/vision-assistant-for-driving/video_llama/datasets/datasets/dataloader_utils.py", line 78, in preload
    self.batch = next(it)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1284, in _get_data
    success, data = self._try_get_data()
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/threading.py", line 316, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt