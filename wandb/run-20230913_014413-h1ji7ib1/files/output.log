| distributed init (rank 0, world 1): env://
Loading VIT
2023-09-13 01:44:20,651 [INFO]
=====  Running Parameters    =====
2023-09-13 01:44:20,652 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 2,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 3e-05,
    "iters_per_epoch": 5,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 100,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "/output/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "video_text_pretrain",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 5,
    "weight_decay": 0.05,
    "world_size": 1
}
2023-09-13 01:44:20,652 [INFO]
======  Dataset Attributes  ======
2023-09-13 01:44:20,652 [INFO]
======== bdd_instruct =======
2023-09-13 01:44:20,652 [INFO] {
    "build_info": {
        "train": {
            "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
            "videos_dir": "/root/BDD-X/"
        },
        "val": {
            "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
            "videos_dir": "/root/BDD-X/"
        }
    },
    "data_type": "video",
    "model_type": "llama_v2",
    "num_video_query_token": 32,
    "text_processor": {
        "train": {
            "name": "blip_caption"
        },
        "val": {
            "name": "blip_caption"
        }
    },
    "tokenizer_name": "/root/ckpt/llama-2-7b-chat-hf",
    "vis_processor": {
        "train": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        },
        "val": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        }
    }
}
2023-09-13 01:44:20,653 [INFO]
======  Model Attributes  ======
2023-09-13 01:44:20,653 [INFO] {
    "arch": "video_llama",
    "ckpt": "/root/ckpt/VL_LLaMA_2_7B_Finetuned.pth",
    "drop_path_rate": 0,
    "equip_audio_branch": false,
    "freeze_qformer": true,
    "freeze_vit": true,
    "frozen_audio_Qformer": true,
    "frozen_llama_proj": false,
    "frozen_video_Qformer": false,
    "fusion_head_layers": 2,
    "fusion_header_type": "seqTransf",
    "image_size": 224,
    "llama_model": "/root/ckpt/llama-2-7b-chat-hf",
    "max_frame_pos": 32,
    "max_txt_len": 320,
    "model_type": "pretrain_llama_v2",
    "num_query_token": 32,
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
2023-09-13 01:44:20,653 [INFO] Building datasets...
Loading VIT Done
Loading Q-Former
2023-09-13 01:44:50,842 [INFO] freeze vision encoder
2023-09-13 01:44:53,984 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth
2023-09-13 01:44:53,998 [INFO] freeze Qformer
2023-09-13 01:44:53,998 [INFO] Loading Q-Former Done
2023-09-13 01:44:53,998 [INFO] Loading LLAMA Tokenizer
Using pad_token, but it is not set yet.
2023-09-13 01:44:54,088 [INFO] Loading LLAMA Model

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:24<00:00, 12.12s/it]
2023-09-13 01:46:13,283 [INFO] Loading LLAMA Done
2023-09-13 01:46:13,283 [INFO] Loading LLAMA proj
2023-09-13 01:46:13,307 [INFO] LLAMA proj is not frozen
2023-09-13 01:46:13,308 [INFO] Loading llama_proj Done
2023-09-13 01:46:14,339 [INFO] video_Qformer is not frozen
2023-09-13 01:46:14,503 [INFO] Start training
Load first Checkpoint: /root/ckpt/VL_LLaMA_2_7B_Finetuned.pth
dict_keys(['train', 'val'])
module.video_query_tokens
module.llama_proj.weight
module.llama_proj.bias
module.video_frame_position_embedding.weight
module.video_Qformer.bert.embeddings.LayerNorm.weight
module.video_Qformer.bert.embeddings.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
ld :  <video_llama.datasets.datasets.dataloader_utils.IterLoader object at 0x7f434c229af0>
2023-09-13 01:46:19,081 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-09-13 01:46:19,082 [INFO] Loaded 10 records for train split from the dataset.
2023-09-13 01:46:19,082 [INFO] Loaded 10 records for val split from the dataset.
2023-09-13 01:46:19,097 [INFO] number of trainable parameters: 22104064
2023-09-13 01:46:19,097 [INFO] Start training epoch 0, 5 iters per inner epoch.
Train: data epoch: [0]  [0/5]  eta: 0:00:14  lr: 0.000001  loss: 2.2720  time: 2.8612  data: 0.0000  max mem: 32999
ld :  <video_llama.datasets.datasets.dataloader_utils.IterLoader object at 0x7f434c229af0>
2023-09-13 01:46:21,962 [INFO] Reducer buckets have been rebuilt in this iteration.
ld :  <video_llama.datasets.datasets.dataloader_utils.IterLoader object at 0x7f434c229af0>
ld :  <video_llama.datasets.datasets.dataloader_utils.IterLoader object at 0x7f434c229af0>
ld :  <video_llama.datasets.datasets.dataloader_utils.IterLoader object at 0x7f434c229af0>
Train: data epoch: [0]  [4/5]  eta: 0:00:00  lr: 0.000024  loss: 1.9258  time: 0.8811  data: 0.0000  max mem: 35726
Train: data epoch: [0] Total time: 0:00:04 (0.8814 s / it)
JBE
Data Loader :  <video_llama.datasets.datasets.dataloader_utils.MultiIterLoader object at 0x7f434c2342e0>
Metric logger :
ld :  <video_llama.datasets.datasets.dataloader_utils.PrefetchLoader object at 0x7f434c234370>
2023-09-13 01:46:23,505 [INFO] Averaged stats: lr: 0.0000  loss: 2.0897
2023-09-13 01:46:23,508 [INFO] Evaluating on val.
==================
ld :  <video_llama.datasets.datasets.dataloader_utils.PrefetchLoader object at 0x7f434c234370>
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/train.py", line 114, in <module>
    main()
  File "/root/vision-assistant-for-driving/train.py", line 110, in main
    runner.train(wandb)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 390, in train
    val_log = self.eval_epoch(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 484, in eval_epoch
    results = self.task.evaluation(model, data_loader)
  File "/root/vision-assistant-for-driving/video_llama/tasks/video_text_pretrain.py", line 46, in evaluation
    if is_dist_avail_and_initialized():
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/_tensor.py", line 930, in __iter__
    raise TypeError("iteration over a 0-d tensor")
TypeError: iteration over a 0-d tensor
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/train.py", line 114, in <module>
    main()
  File "/root/vision-assistant-for-driving/train.py", line 110, in main
    runner.train(wandb)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 390, in train
    val_log = self.eval_epoch(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 484, in eval_epoch
    results = self.task.evaluation(model, data_loader)
  File "/root/vision-assistant-for-driving/video_llama/tasks/video_text_pretrain.py", line 46, in evaluation
    if is_dist_avail_and_initialized():
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/_tensor.py", line 930, in __iter__
    raise TypeError("iteration over a 0-d tensor")
TypeError: iteration over a 0-d tensor
samples : {'input_ids': tensor([[    1, 29871,    13,    13,  2277, 29937, 29950,  7889, 29901,   529,
         15167, 29958, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000,  1533, 15167, 29958,   450,  4863,  3743,
         29871, 29947, 16608,  4559, 29881,   472, 29871, 29900, 29889, 29900,
         29892, 29871, 29945, 29889, 29900, 29892, 29871, 29896, 29900, 29889,
         29900, 29892, 29871, 29896, 29945, 29889, 29900, 29892, 29871, 29906,
         29900, 29889, 29896, 29892, 29871, 29906, 29945, 29889, 29896, 29892,
         29871, 29941, 29900, 29889, 29896, 29892, 29871, 29941, 29945, 29889,
         29896,  6923, 29889, 12027,  7420,   278,  7156, 29915, 29879,  6030,
           297,   278,  9088, 29892,  3704,   278,  6520,  3618,   322,   278,
         12469, 29889,    13,  2277, 29937,  7900, 22137, 29901,  3824, 29892,
           278,   321,  1484, 29899,  4287, 12402,  1078,   263,  2821,  4272,
         11952, 29892,  7344,   292,   263, 13747,  6210,  1550,  7952,   292,
           297,   967,   301,  1662, 29892, 23941,  4226,  4272, 19500,  5855,
         29889, 29871,    13,    13,  9190, 29892,   278,  7156,  3732,   263,
           301,  1662,  1735,   304,   278,  2175, 29889,   910,  3158,   338,
          5517,  2861,   304,   263,  1559,  4153, 14432,   297,   278,  1857,
           301,  1662, 29889,   450,   767, 12932,   369,   304,   278,  2175,
           301,  1662,   338, 23511,  8283,   408,   278,   301,  1662,   338,
          8967,   304,   367,  2821, 29892,  5662,  3864,   278,  2145,   950,
         10597,  6728,   310,   278,   321,  1484, 29899,  4287,  1623,   278,
         11952, 29889,    13,    13, 11760, 29892,   278,   321,  1484, 29899,
          4287, 13501,   385, 17686,   988,   278, 12469,  3578, 12169,  2654,
         29889,   512,  2933,   304,   445,  1735, 29892,   278,  7156,  5544,
         14981, 23522,   278,  1559,   304,   263,  4866,  5040, 29892,   752,
          5890,   411, 12469,  6865,   322,  1072,  8250, 29889,   450,  2654,
          3578, 14661,   727,  1122,   367,  4891, 12469,   470,  8939,   342,
           374,   550,   393,   278,  7156,  4225,   304,  7709,   304, 29889,
            13,    13, 12881,   635, 29892,   278,  1559,  9242,  5073,   653,
           472,   278, 17686,   408,   278, 12469,  3578, 18172,   304,  1510,
           278,  2654,  7182, 29889,   910,  3785,   310,   297, 10072,  9432,
         29879,   278,  7156, 29915, 29879,   594,  2276,   663,   304, 12469,
         14243, 29892,   408,  8401,   472,   263,  2654,  3578,  1033,  1121,
           297,  1035, 16719,   470,  6584,  1997,   583, 29889,  7133,   445,
           931, 29892,   278,  7156,  1795,   367,  8454,   363,   738,  3620,
           297,   278, 12469,  3578,   470, 20810, 18647,   322,  8939,   342,
           374,   550, 10223,   292,   363,  1009,  2446,  4337, 29889,    13,
          2277, 29937],
        [    1, 29871,    13,    13,  2277, 29937, 29950,  7889, 29901,   529,
         15167, 29958, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000,  1533, 15167, 29958,   450,  4863,  3743,
         29871, 29947, 16608,  4559, 29881,   472, 29871, 29900, 29889, 29900,
         29892, 29871, 29945, 29889, 29900, 29892, 29871, 29896, 29900, 29889,
         29900, 29892, 29871, 29896, 29945, 29889, 29900, 29892, 29871, 29906,
         29900, 29889, 29900, 29892, 29871, 29906, 29945, 29889, 29900, 29892,
         29871, 29941, 29900, 29889, 29896, 29892, 29871, 29941, 29945, 29889,
         29896,  6923, 29889,  4587,   571,   385,  8252,   310,   278, 19500,
          9088,  9132, 29889,    13,  2277, 29937,  7900, 22137, 29901, 17044,
           449,   278,  6790, 14385, 29892,   278,   321,  1484, 29899,  4287,
          9242, 14089,   287,   373,   278,  2625,   310,   278,  6520, 29889,
          1670,   338,   694,  2702, 14881,   411,   916, 24413,   470,  8939,
           342,   374,   550,  2645,   445,  9088, 29892,   408,   278,  1559,
           338,  5073,   653,   363,   278,  4152,  1017,   310,   278,  4944,
           931,  3785, 29889,   450,  7156,   310,   278,   321,  1484, 29899,
          4287,  1122,   505, 14089,   287,   304,  1065,   385,  4589,   392,
           470,  3763, 10534,   297,   278,  1559, 29889,   910, 10483, 14088,
           263,  1209,   573,  4464,   310, 19500, 29892,   694,   916,  8820,
           470,   767, 12932,   874,   526,  8283,   491,   278,  7156, 29889,
           450,  1559, 29915, 29879,  5073,   653,  2602,   373,   278,  2625,
           310,   278,  6520,  7344, 29879, 12469,  4972,   322,   947,   451,
           766,  6685,   738,   373, 17696, 10298,   297,   278, 19500,  9088,
         29889,   450, 18830,  5177,  1122, 13100, 29892,   541,  1728,  5684,
          2472,   470,  2702, 29879,  4944, 29892,   372,   338, 12023,   393,
           278,  4038,  2820,   278, 14089,   287,  1559,   338, 15662,   310,
           263,  4943,  6520,  2975, 10483, 29889,    13,  2277, 29937,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  2277, 29937,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2277, 29937,  7900, 22137, 29901,  3824, 29892,
           278,   321,  1484, 29899,  4287, 12402,  1078,   263,  2821,  4272,
         11952, 29892,  7344,   292,   263, 13747,  6210,  1550,  7952,   292,
           297,   967,   301,  1662, 29892, 23941,  4226,  4272, 19500,  5855,
         29889, 29871,    13,    13,  9190, 29892,   278,  7156,  3732,   263,
           301,  1662,  1735,   304,   278,  2175, 29889,   910,  3158,   338,
          5517,  2861,   304,   263,  1559,  4153, 14432,   297,   278,  1857,
           301,  1662, 29889,   450,   767, 12932,   369,   304,   278,  2175,
           301,  1662,   338, 23511,  8283,   408,   278,   301,  1662,   338,
          8967,   304,   367,  2821, 29892,  5662,  3864,   278,  2145,   950,
         10597,  6728,   310,   278,   321,  1484, 29899,  4287,  1623,   278,
         11952, 29889,    13,    13, 11760, 29892,   278,   321,  1484, 29899,
          4287, 13501,   385, 17686,   988,   278, 12469,  3578, 12169,  2654,
         29889,   512,  2933,   304,   445,  1735, 29892,   278,  7156,  5544,
         14981, 23522,   278,  1559,   304,   263,  4866,  5040, 29892,   752,
          5890,   411, 12469,  6865,   322,  1072,  8250, 29889,   450,  2654,
          3578, 14661,   727,  1122,   367,  4891, 12469,   470,  8939,   342,
           374,   550,   393,   278,  7156,  4225,   304,  7709,   304, 29889,
            13,    13, 12881,   635, 29892,   278,  1559,  9242,  5073,   653,
           472,   278, 17686,   408,   278, 12469,  3578, 18172,   304,  1510,
           278,  2654,  7182, 29889,   910,  3785,   310,   297, 10072,  9432,
         29879,   278,  7156, 29915, 29879,   594,  2276,   663,   304, 12469,
         14243, 29892,   408,  8401,   472,   263,  2654,  3578,  1033,  1121,
           297,  1035, 16719,   470,  6584,  1997,   583, 29889,  7133,   445,
           931, 29892,   278,  7156,  1795,   367,  8454,   363,   738,  3620,
           297,   278, 12469,  3578,   470, 20810, 18647,   322,  8939,   342,
           374,   550, 10223,   292,   363,  1009,  2446,  4337, 29889,    13,
          2277, 29937],
        [ -100,  -100,  -100,  -100,  2277, 29937,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  2277, 29937,  7900, 22137, 29901, 17044,
           449,   278,  6790, 14385, 29892,   278,   321,  1484, 29899,  4287,
          9242, 14089,   287,   373,   278,  2625,   310,   278,  6520, 29889,
          1670,   338,   694,  2702, 14881,   411,   916, 24413,   470,  8939,
           342,   374,   550,  2645,   445,  9088, 29892,   408,   278,  1559,
           338,  5073,   653,   363,   278,  4152,  1017,   310,   278,  4944,
           931,  3785, 29889,   450,  7156,   310,   278,   321,  1484, 29899,
          4287,  1122,   505, 14089,   287,   304,  1065,   385,  4589,   392,
           470,  3763, 10534,   297,   278,  1559, 29889,   910, 10483, 14088,
           263,  1209,   573,  4464,   310, 19500, 29892,   694,   916,  8820,
           470,   767, 12932,   874,   526,  8283,   491,   278,  7156, 29889,
           450,  1559, 29915, 29879,  5073,   653,  2602,   373,   278,  2625,
           310,   278,  6520,  7344, 29879, 12469,  4972,   322,   947,   451,
           766,  6685,   738,   373, 17696, 10298,   297,   278, 19500,  9088,
         29889,   450, 18830,  5177,  1122, 13100, 29892,   541,  1728,  5684,
          2472,   470,  2702, 29879,  4944, 29892,   372,   338, 12023,   393,
           278,  4038,  2820,   278, 14089,   287,  1559,   338, 15662,   310,
           263,  4943,  6520,  2975, 10483, 29889,    13,  2277, 29937,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100]], device='cuda:0'), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False]], device='cuda:0'), 'images': tensor([[[[[-0.3178, -1.1937, -1.7339,  ..., -1.6025, -1.6171, -1.5587],
           [-0.2740, -1.1353, -1.6755,  ..., -1.6025, -1.5879, -1.5587],
           [-0.2010, -1.0477, -1.6171,  ..., -1.6025, -1.5879, -1.5733],
           ...,
           [-1.1937, -1.1937, -1.1937,  ..., -1.5149, -1.5149, -1.5295],
           [-1.2083, -1.1937, -1.1937,  ..., -1.5149, -1.5149, -1.5149],
           [-1.1937, -1.2083, -1.1937,  ..., -1.5295, -1.5149, -1.5149]],
          [[-1.1207, -1.1353, -1.1499,  ..., -1.6463, -1.6317, -1.6025],
           [-1.0769, -1.0769, -1.1061,  ..., -1.6463, -1.6317, -1.6025],
           [-0.9748, -0.9893, -1.0331,  ..., -1.6463, -1.6317, -1.6171],
           ...,
           [-1.2959, -1.2959, -1.2813,  ..., -1.5733, -1.5587, -1.5295],
           [-1.2813, -1.2667, -1.2667,  ..., -1.5733, -1.5587, -1.5441],
           [-1.2521, -1.2375, -1.2521,  ..., -1.5587, -1.5733, -1.5733]],
          [[-1.7339, -1.7339, -1.7193,  ..., -1.6317, -1.6317, -1.6171],
           [-1.7339, -1.7339, -1.7339,  ..., -1.6317, -1.6317, -1.6171],
           [-1.7193, -1.7193, -1.7047,  ..., -1.6171, -1.6171, -1.6317],
           ...,
           [-1.2083, -1.1645, -1.1499,  ..., -1.3543, -1.3835, -1.3689],
           [-1.2083, -1.1791, -1.1645,  ..., -1.3835, -1.4127, -1.3981],
           [-1.2229, -1.1937, -1.1791,  ..., -1.4127, -1.4127, -1.4127]],
          ...,
          [[-1.2959, -1.2521, -1.2229,  ..., -1.6609, -1.6609, -1.6317],
           [-1.2959, -1.2521, -1.2229,  ..., -1.6609, -1.6609, -1.6463],
           [-1.3105, -1.2521, -1.2375,  ..., -1.6463, -1.6463, -1.6317],
           ...,
           [-0.7704, -0.7704, -0.7558,  ..., -0.7266, -0.7412, -0.7850],
           [-0.9456, -0.9456, -0.9456,  ..., -0.7412, -0.7412, -0.7850],
           [-1.0185, -1.0331, -1.0185,  ..., -0.7412, -0.7412, -0.7850]],
          [[-1.2229, -1.2229, -1.1791,  ..., -1.6171, -1.6317, -1.6463],
           [-1.2667, -1.2521, -1.1645,  ..., -1.6025, -1.6317, -1.6463],
           [-1.2959, -1.2813, -1.1353,  ..., -1.5879, -1.6171, -1.6463],
           ...,
           [-0.5222, -0.4930, -0.4492,  ..., -0.7996, -0.7996, -0.8142],
           [-0.5660, -0.5222, -0.4638,  ..., -0.8142, -0.8288, -0.8288],
           [-0.5806, -0.5222, -0.4492,  ..., -0.8288, -0.8434, -0.8142]],
          [[-1.2375, -1.2083, -1.1791,  ..., -1.6317, -1.6463, -1.6463],
           [-1.2375, -1.2375, -1.1645,  ..., -1.6317, -1.6463, -1.6463],
           [-1.2521, -1.2375, -1.1353,  ..., -1.6317, -1.6317, -1.6317],
           ...,
           [-0.4638, -0.4200, -0.4200,  ..., -0.8288, -0.8142, -0.7704],
           [-0.5222, -0.4784, -0.4200,  ..., -0.7996, -0.7996, -0.7704],
           [-0.5514, -0.5076, -0.4346,  ..., -0.7704, -0.7704, -0.7850]]],
         [[[ 0.0488, -1.0017, -1.6320,  ..., -1.6921, -1.7071, -1.6771],
           [ 0.1089, -0.9417, -1.6020,  ..., -1.6771, -1.6771, -1.6771],
           [ 0.1839, -0.8816, -1.5570,  ..., -1.6771, -1.6771, -1.6921],
           ...,
           [-1.1368, -1.1368, -1.1368,  ..., -1.6020, -1.6020, -1.6170],
           [-1.1518, -1.1368, -1.1368,  ..., -1.6020, -1.6020, -1.6020],
           [-1.1368, -1.1518, -1.1518,  ..., -1.6170, -1.6170, -1.6020]],
          [[-1.4069, -1.4219, -1.4519,  ..., -1.6921, -1.6921, -1.6921],
           [-1.3919, -1.4069, -1.4369,  ..., -1.6771, -1.6771, -1.6921],
           [-1.3469, -1.3769, -1.3919,  ..., -1.6921, -1.6921, -1.6771],
           ...,
           [-1.4069, -1.4069, -1.3919,  ..., -1.6170, -1.6020, -1.5870],
           [-1.3769, -1.3769, -1.3919,  ..., -1.6020, -1.6170, -1.6170],
           [-1.3469, -1.3619, -1.3769,  ..., -1.6020, -1.6170, -1.6470]],
          [[-1.7071, -1.7071, -1.6921,  ..., -1.6771, -1.6921, -1.7071],
           [-1.7071, -1.7071, -1.6921,  ..., -1.6771, -1.6921, -1.7071],
           [-1.6921, -1.6921, -1.6921,  ..., -1.6621, -1.6771, -1.7071],
           ...,
           [-1.2869, -1.2568, -1.2418,  ..., -1.5270, -1.5420, -1.5270],
           [-1.2869, -1.2718, -1.2568,  ..., -1.5120, -1.5270, -1.5270],
           [-1.3019, -1.2718, -1.2568,  ..., -1.4970, -1.4970, -1.4970]],
          ...,
          [[-1.4519, -1.4369, -1.4219,  ..., -1.6771, -1.6921, -1.6771],
           [-1.4820, -1.4369, -1.4219,  ..., -1.6621, -1.6771, -1.6921],
           [-1.4970, -1.4519, -1.4369,  ..., -1.6470, -1.6771, -1.6921],
           ...,
           [-0.9267, -0.9267, -0.9117,  ..., -1.0467, -1.0317, -1.0167],
           [-1.0918, -1.1068, -1.0918,  ..., -1.0617, -1.0317, -1.0317],
           [-1.1668, -1.1818, -1.1668,  ..., -1.0467, -1.0317, -1.0467]],
          [[-1.4219, -1.4669, -1.4369,  ..., -1.6621, -1.6771, -1.6921],
           [-1.4369, -1.4970, -1.4219,  ..., -1.6320, -1.6621, -1.6921],
           [-1.4669, -1.5120, -1.3919,  ..., -1.6170, -1.6470, -1.6921],
           ...,
           [-0.6865, -0.6415, -0.6115,  ..., -1.0467, -1.0467, -1.0617],
           [-0.7166, -0.6715, -0.6115,  ..., -1.0617, -1.0767, -1.0767],
           [-0.7166, -0.6565, -0.5965,  ..., -1.0767, -1.0918, -1.0617]],
          [[-1.4219, -1.4369, -1.4069,  ..., -1.6771, -1.6921, -1.6921],
           [-1.4519, -1.4669, -1.4069,  ..., -1.6771, -1.6921, -1.6771],
           [-1.4669, -1.4970, -1.3919,  ..., -1.6771, -1.6921, -1.6921],
           ...,
           [-0.6115, -0.5665, -0.5665,  ..., -1.0767, -1.0617, -1.0767],
           [-0.6865, -0.6265, -0.5665,  ..., -1.0767, -1.0767, -1.0617],
           [-0.7166, -0.6565, -0.5815,  ..., -1.0767, -1.0767, -1.0617]]],
         [[[-0.9399, -1.2243, -1.4518,  ..., -1.4660, -1.4660, -1.4233],
           [-0.9256, -1.1816, -1.4233,  ..., -1.4518, -1.4518, -1.4233],
           [-0.9256, -1.1389, -1.4091,  ..., -1.4518, -1.4376, -1.4376],
           ...,
           [-1.0394, -1.0394, -1.0394,  ..., -1.4376, -1.4376, -1.4518],
           [-1.0536, -1.0394, -1.0394,  ..., -1.4376, -1.4376, -1.4376],
           [-1.0394, -1.0536, -1.0536,  ..., -1.4518, -1.4518, -1.4518]],
          [[-1.2527, -1.3096, -1.3522,  ..., -1.4091, -1.4233, -1.4376],
           [-1.2527, -1.2954, -1.3380,  ..., -1.4091, -1.4233, -1.4233],
           [-1.2243, -1.2669, -1.3096,  ..., -1.4233, -1.4233, -1.4376],
           ...,
           [-1.2527, -1.2669, -1.2669,  ..., -1.4091, -1.4233, -1.4376],
           [-1.2243, -1.2527, -1.2669,  ..., -1.4091, -1.4233, -1.4233],
           [-1.2100, -1.2385, -1.2527,  ..., -1.4091, -1.4233, -1.4376]],
          [[-1.4376, -1.4660, -1.4660,  ..., -1.4660, -1.4802, -1.4660],
           [-1.4660, -1.4660, -1.4660,  ..., -1.4660, -1.4660, -1.4802],
           [-1.4802, -1.4660, -1.4660,  ..., -1.4518, -1.4660, -1.4802],
           ...,
           [-1.1674, -1.1532, -1.1389,  ..., -1.4233, -1.4518, -1.4376],
           [-1.1958, -1.1674, -1.1532,  ..., -1.4376, -1.4376, -1.4376],
           [-1.2100, -1.1674, -1.1532,  ..., -1.4376, -1.4233, -1.4233]],
          ...,
          [[-1.3665, -1.3807, -1.3665,  ..., -1.4518, -1.4660, -1.4802],
           [-1.3522, -1.3665, -1.3522,  ..., -1.4518, -1.4518, -1.4660],
           [-1.3380, -1.3665, -1.3522,  ..., -1.4376, -1.4376, -1.4376],
           ...,
           [-0.9114, -0.9256, -0.8972,  ..., -1.2385, -1.2243, -1.2100],
           [-1.0963, -1.1105, -1.0963,  ..., -1.2527, -1.2243, -1.1816],
           [-1.1958, -1.1958, -1.1958,  ..., -1.2527, -1.2100, -1.1674]],
          [[-1.3238, -1.3380, -1.2954,  ..., -1.4233, -1.4376, -1.4802],
           [-1.3380, -1.3949, -1.3096,  ..., -1.4091, -1.4376, -1.4802],
           [-1.3665, -1.4518, -1.3522,  ..., -1.4091, -1.4376, -1.4802],
           ...,
           [-0.6839, -0.6270, -0.5986,  ..., -1.2385, -1.2669, -1.3096],
           [-0.7123, -0.6555, -0.5986,  ..., -1.2811, -1.3096, -1.3238],
           [-0.7123, -0.6555, -0.5844,  ..., -1.3238, -1.3380, -1.3096]],
          [[-1.3522, -1.4091, -1.3949,  ..., -1.4376, -1.4518, -1.4518],
           [-1.3522, -1.4091, -1.3522,  ..., -1.4376, -1.4518, -1.4376],
           [-1.3380, -1.3949, -1.3238,  ..., -1.4376, -1.4376, -1.4518],
           ...,
           [-0.5986, -0.5559, -0.5559,  ..., -1.3096, -1.2954, -1.2527],
           [-0.6697, -0.6128, -0.5701,  ..., -1.3238, -1.2954, -1.2527],
           [-0.6981, -0.6412, -0.5701,  ..., -1.3096, -1.2954, -1.2527]]]],
        [[[[-0.7996, -0.7996, -0.8872,  ..., -0.9602, -0.9164, -0.8580],
           [-0.8288, -0.8434, -0.9164,  ..., -0.9456, -0.9018, -0.7850],
           [-0.8580, -0.8872, -0.9748,  ..., -0.9018, -0.8434, -0.7120],
           ...,
           [-0.7704, -0.7558, -0.7120,  ..., -1.4419, -1.4565, -1.4565],
           [-0.7704, -0.7558, -0.7120,  ..., -1.4273, -1.4419, -1.4419],
           [-0.7412, -0.7412, -0.6974,  ..., -1.4273, -1.4419, -1.4273]],
          [[-0.8726, -0.8872, -0.9164,  ..., -0.9748, -0.9164, -0.8872],
           [-0.8872, -0.9018, -0.9456,  ..., -0.9310, -0.9164, -0.8142],
           [-0.8872, -0.9018, -0.9748,  ..., -0.8580, -0.8434, -0.6974],
           ...,
           [-0.7412, -0.7558, -0.7120,  ..., -1.4127, -1.4273, -1.4273],
           [-0.7412, -0.7266, -0.6974,  ..., -1.4127, -1.4127, -1.4127],
           [-0.7412, -0.7120, -0.6828,  ..., -1.4127, -1.4273, -1.4127]],
          [[-0.8580, -0.8434, -0.8872,  ..., -0.9893, -0.9310, -0.9164],
           [-0.8434, -0.8726, -0.9310,  ..., -0.9602, -0.9164, -0.8580],
           [-0.8580, -0.9018, -0.9893,  ..., -0.8726, -0.8142, -0.7266],
           ...,
           [-0.7266, -0.7266, -0.6974,  ..., -1.4273, -1.4419, -1.4419],
           [-0.7412, -0.7120, -0.6974,  ..., -1.4273, -1.4419, -1.4565],
           [-0.7412, -0.7120, -0.6974,  ..., -1.4273, -1.4565, -1.4711]],
          ...,
          [[-0.8434, -0.8872, -0.9164,  ..., -0.9748, -0.9456, -0.8580],
           [-0.8580, -0.8872, -0.9602,  ..., -0.9164, -0.9164, -0.8142],
           [-0.8872, -0.9164, -1.0185,  ..., -0.8288, -0.8288, -0.7558],
           ...,
           [-0.7120, -0.7120, -0.6974,  ..., -1.4565, -1.4711, -1.5003],
           [-0.7120, -0.6974, -0.6682,  ..., -1.4711, -1.4711, -1.4565],
           [-0.7120, -0.6828, -0.6536,  ..., -1.4711, -1.4565, -1.4419]],
          [[-0.8580, -0.8434, -0.8872,  ..., -0.9748, -0.9164, -0.8872],
           [-0.8288, -0.8580, -0.8872,  ..., -0.9602, -0.9018, -0.8288],
           [-0.8434, -0.8872, -0.9164,  ..., -0.8872, -0.8142, -0.7266],
           ...,
           [-1.2229, -1.2083, -1.2229,  ..., -1.4419, -1.4419, -1.4419],
           [-1.2229, -1.2083, -1.2229,  ..., -1.4273, -1.4273, -1.4273],
           [-1.2229, -1.1791, -1.2083,  ..., -1.3981, -1.4273, -1.4419]],
          [[-0.9018, -0.8872, -0.9018,  ..., -0.9893, -0.9164, -0.9018],
           [-0.8872, -0.8872, -0.9164,  ..., -0.9456, -0.9018, -0.8580],
           [-0.8726, -0.8872, -0.9456,  ..., -0.8872, -0.8434, -0.7558],
           ...,
           [-0.8434, -0.7558, -0.6682,  ..., -1.4127, -1.4273, -1.4565],
           [-0.8288, -0.7412, -0.6536,  ..., -1.4127, -1.4273, -1.4419],
           [-0.8288, -0.7266, -0.6682,  ..., -1.4127, -1.4419, -1.4565]]],
         [[[-1.2869, -1.3019, -1.3469,  ..., -1.0767, -1.0167, -0.9567],
           [-1.3169, -1.3319, -1.3769,  ..., -1.0467, -0.9867, -0.8967],
           [-1.3319, -1.3469, -1.3919,  ..., -0.9867, -0.9267, -0.8066],
           ...,
           [-1.2268, -1.2118, -1.1968,  ..., -1.3469, -1.3919, -1.3919],
           [-1.2268, -1.2118, -1.1818,  ..., -1.3319, -1.3919, -1.3769],
           [-1.1968, -1.1968, -1.1818,  ..., -1.3169, -1.3769, -1.3619]],
          [[-1.2718, -1.2869, -1.3469,  ..., -1.0617, -0.9717, -0.9417],
           [-1.2869, -1.2869, -1.3619,  ..., -1.0317, -0.9867, -0.8816],
           [-1.3019, -1.3019, -1.3769,  ..., -0.9867, -0.9417, -0.8066],
           ...,
           [-1.2418, -1.2568, -1.2268,  ..., -1.3769, -1.3919, -1.3919],
           [-1.2418, -1.2118, -1.1968,  ..., -1.3769, -1.3769, -1.3769],
           [-1.2418, -1.1968, -1.1818,  ..., -1.3769, -1.3919, -1.3769]],
          [[-1.3019, -1.2869, -1.3019,  ..., -1.0918, -1.0317, -1.0017],
           [-1.2869, -1.3169, -1.3469,  ..., -1.0467, -1.0017, -0.9417],
           [-1.3019, -1.3469, -1.4069,  ..., -0.9717, -0.9117, -0.8216],
           ...,
           [-1.2268, -1.2118, -1.1818,  ..., -1.3469, -1.4069, -1.4219],
           [-1.2418, -1.2118, -1.1818,  ..., -1.3469, -1.4069, -1.4069],
           [-1.2418, -1.1968, -1.1818,  ..., -1.3319, -1.3919, -1.3919]],
          ...,
          [[-1.2869, -1.3319, -1.3619,  ..., -1.1068, -1.0617, -0.9717],
           [-1.2869, -1.3169, -1.3769,  ..., -1.0467, -1.0017, -0.9117],
           [-1.2718, -1.3169, -1.4069,  ..., -0.9567, -0.9117, -0.8366],
           ...,
           [-1.2418, -1.2568, -1.2118,  ..., -1.3319, -1.3919, -1.4069],
           [-1.2418, -1.2268, -1.1968,  ..., -1.3319, -1.3769, -1.3619],
           [-1.2418, -1.1968, -1.1818,  ..., -1.3319, -1.3769, -1.3469]],
          [[-1.3019, -1.2869, -1.3319,  ..., -1.0617, -1.0167, -0.9717],
           [-1.2869, -1.2869, -1.3319,  ..., -1.0317, -0.9867, -0.9117],
           [-1.2718, -1.3019, -1.3619,  ..., -0.9567, -0.9117, -0.8066],
           ...,
           [-1.3619, -1.3469, -1.3469,  ..., -1.3469, -1.3769, -1.3769],
           [-1.3469, -1.3169, -1.3469,  ..., -1.3319, -1.3619, -1.3619],
           [-1.3469, -1.2869, -1.3319,  ..., -1.3019, -1.3619, -1.3769]],
          [[-1.3019, -1.2869, -1.3019,  ..., -1.0918, -1.0167, -0.9867],
           [-1.2869, -1.2869, -1.3319,  ..., -1.0467, -1.0017, -0.9417],
           [-1.2718, -1.2869, -1.3919,  ..., -0.9717, -0.9267, -0.8366],
           ...,
           [-1.2869, -1.1818, -1.1368,  ..., -1.3169, -1.3769, -1.3919],
           [-1.2869, -1.1818, -1.1368,  ..., -1.3019, -1.3619, -1.3769],
           [-1.2718, -1.1668, -1.1368,  ..., -1.3019, -1.3469, -1.3469]]],
         [[[-1.3096, -1.3238, -1.2811,  ..., -0.9825, -0.9541, -0.9114],
           [-1.2811, -1.2954, -1.2811,  ..., -0.9683, -0.9541, -0.8688],
           [-1.2527, -1.2669, -1.2811,  ..., -0.9541, -0.9114, -0.7977],
           ...,
           [-1.2243, -1.2243, -1.1958,  ..., -1.1958, -1.2385, -1.2385],
           [-1.2243, -1.2243, -1.1816,  ..., -1.1958, -1.2243, -1.2243],
           [-1.2100, -1.2100, -1.1816,  ..., -1.1816, -1.2243, -1.2100]],
          [[-1.2385, -1.2527, -1.3096,  ..., -1.0394, -0.9256, -0.9114],
           [-1.2527, -1.2669, -1.3096,  ..., -1.0252, -0.9683, -0.8688],
           [-1.2669, -1.2669, -1.3238,  ..., -0.9967, -0.9541, -0.8261],
           ...,
           [-1.2385, -1.2385, -1.2385,  ..., -1.1958, -1.2243, -1.2243],
           [-1.2385, -1.2243, -1.2243,  ..., -1.1958, -1.2243, -1.2100],
           [-1.2385, -1.2100, -1.2100,  ..., -1.1958, -1.2243, -1.2100]],
          [[-1.2527, -1.2527, -1.2385,  ..., -1.0678, -1.0110, -0.9967],
           [-1.2243, -1.2385, -1.2811,  ..., -1.0394, -0.9967, -0.9399],
           [-1.2243, -1.2527, -1.3238,  ..., -0.9967, -0.9399, -0.8403],
           ...,
           [-1.2385, -1.2385, -1.1958,  ..., -1.1958, -1.2385, -1.2527],
           [-1.2243, -1.2100, -1.1816,  ..., -1.1958, -1.2385, -1.2527],
           [-1.2385, -1.2100, -1.1816,  ..., -1.1958, -1.2385, -1.2527]],
          ...,
          [[-1.2527, -1.2669, -1.3096,  ..., -1.0536, -0.9399, -0.8545],
           [-1.2243, -1.2527, -1.3238,  ..., -0.9967, -0.9256, -0.8403],
           [-1.2100, -1.2385, -1.3380,  ..., -0.9399, -0.8830, -0.8119],
           ...,
           [-1.2243, -1.2385, -1.1958,  ..., -1.1816, -1.2100, -1.2385],
           [-1.2243, -1.2100, -1.1816,  ..., -1.1958, -1.2100, -1.1958],
           [-1.2243, -1.1816, -1.1674,  ..., -1.1958, -1.1958, -1.1674]],
          [[-1.2527, -1.2385, -1.2669,  ..., -1.0536, -0.9967, -0.9541],
           [-1.2243, -1.2243, -1.2811,  ..., -1.0252, -0.9683, -0.8972],
           [-1.1958, -1.2243, -1.2954,  ..., -0.9541, -0.8830, -0.7977],
           ...,
           [-1.2243, -1.2243, -1.2243,  ..., -1.1958, -1.2243, -1.2243],
           [-1.2243, -1.1958, -1.2243,  ..., -1.1816, -1.2100, -1.2100],
           [-1.2243, -1.1674, -1.2100,  ..., -1.1674, -1.2100, -1.2243]],
          [[-1.3096, -1.3096, -1.2811,  ..., -1.0536, -0.9541, -0.9399],
           [-1.2669, -1.2669, -1.3096,  ..., -1.0252, -0.9541, -0.9256],
           [-1.2385, -1.2385, -1.3238,  ..., -0.9825, -0.9114, -0.8403],
           ...,
           [-1.2385, -1.1389, -1.1247,  ..., -1.1674, -1.2243, -1.2385],
           [-1.2385, -1.1389, -1.1389,  ..., -1.1674, -1.2100, -1.2243],
           [-1.2527, -1.1532, -1.1674,  ..., -1.1674, -1.2100, -1.2100]]]]],
       device='cuda:0'), 'conv_type': 'multi'}
{'input_ids': tensor([[    1, 29871,    13,    13,  2277, 29937, 29950,  7889, 29901,   529,
         15167, 29958, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000,  1533, 15167, 29958,   450,  4863,  3743,
         29871, 29947, 16608,  4559, 29881,   472, 29871, 29900, 29889, 29900,
         29892, 29871, 29945, 29889, 29900, 29892, 29871, 29896, 29900, 29889,
         29900, 29892, 29871, 29896, 29945, 29889, 29900, 29892, 29871, 29906,
         29900, 29889, 29896, 29892, 29871, 29906, 29945, 29889, 29896, 29892,
         29871, 29941, 29900, 29889, 29896, 29892, 29871, 29941, 29945, 29889,
         29896,  6923, 29889, 12027,  7420,   278,  7156, 29915, 29879,  6030,
           297,   278,  9088, 29892,  3704,   278,  6520,  3618,   322,   278,
         12469, 29889,    13,  2277, 29937,  7900, 22137, 29901,  3824, 29892,
           278,   321,  1484, 29899,  4287, 12402,  1078,   263,  2821,  4272,
         11952, 29892,  7344,   292,   263, 13747,  6210,  1550,  7952,   292,
           297,   967,   301,  1662, 29892, 23941,  4226,  4272, 19500,  5855,
         29889, 29871,    13,    13,  9190, 29892,   278,  7156,  3732,   263,
           301,  1662,  1735,   304,   278,  2175, 29889,   910,  3158,   338,
          5517,  2861,   304,   263,  1559,  4153, 14432,   297,   278,  1857,
           301,  1662, 29889,   450,   767, 12932,   369,   304,   278,  2175,
           301,  1662,   338, 23511,  8283,   408,   278,   301,  1662,   338,
          8967,   304,   367,  2821, 29892,  5662,  3864,   278,  2145,   950,
         10597,  6728,   310,   278,   321,  1484, 29899,  4287,  1623,   278,
         11952, 29889,    13,    13, 11760, 29892,   278,   321,  1484, 29899,
          4287, 13501,   385, 17686,   988,   278, 12469,  3578, 12169,  2654,
         29889,   512,  2933,   304,   445,  1735, 29892,   278,  7156,  5544,
         14981, 23522,   278,  1559,   304,   263,  4866,  5040, 29892,   752,
          5890,   411, 12469,  6865,   322,  1072,  8250, 29889,   450,  2654,
          3578, 14661,   727,  1122,   367,  4891, 12469,   470,  8939,   342,
           374,   550,   393,   278,  7156,  4225,   304,  7709,   304, 29889,
            13,    13, 12881,   635, 29892,   278,  1559,  9242,  5073,   653,
           472,   278, 17686,   408,   278, 12469,  3578, 18172,   304,  1510,
           278,  2654,  7182, 29889,   910,  3785,   310,   297, 10072,  9432,
         29879,   278,  7156, 29915, 29879,   594,  2276,   663,   304, 12469,
         14243, 29892,   408,  8401,   472,   263,  2654,  3578,  1033,  1121,
           297,  1035, 16719,   470,  6584,  1997,   583, 29889,  7133,   445,
           931, 29892,   278,  7156,  1795,   367,  8454,   363,   738,  3620,
           297,   278, 12469,  3578,   470, 20810, 18647,   322,  8939,   342,
           374,   550, 10223,   292,   363,  1009,  2446,  4337, 29889,    13,
          2277, 29937],
        [    1, 29871,    13,    13,  2277, 29937, 29950,  7889, 29901,   529,
         15167, 29958, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,
         32000, 32000, 32000, 32000,  1533, 15167, 29958,   450,  4863,  3743,
         29871, 29947, 16608,  4559, 29881,   472, 29871, 29900, 29889, 29900,
         29892, 29871, 29945, 29889, 29900, 29892, 29871, 29896, 29900, 29889,
         29900, 29892, 29871, 29896, 29945, 29889, 29900, 29892, 29871, 29906,
         29900, 29889, 29900, 29892, 29871, 29906, 29945, 29889, 29900, 29892,
         29871, 29941, 29900, 29889, 29896, 29892, 29871, 29941, 29945, 29889,
         29896,  6923, 29889,  4587,   571,   385,  8252,   310,   278, 19500,
          9088,  9132, 29889,    13,  2277, 29937,  7900, 22137, 29901, 17044,
           449,   278,  6790, 14385, 29892,   278,   321,  1484, 29899,  4287,
          9242, 14089,   287,   373,   278,  2625,   310,   278,  6520, 29889,
          1670,   338,   694,  2702, 14881,   411,   916, 24413,   470,  8939,
           342,   374,   550,  2645,   445,  9088, 29892,   408,   278,  1559,
           338,  5073,   653,   363,   278,  4152,  1017,   310,   278,  4944,
           931,  3785, 29889,   450,  7156,   310,   278,   321,  1484, 29899,
          4287,  1122,   505, 14089,   287,   304,  1065,   385,  4589,   392,
           470,  3763, 10534,   297,   278,  1559, 29889,   910, 10483, 14088,
           263,  1209,   573,  4464,   310, 19500, 29892,   694,   916,  8820,
           470,   767, 12932,   874,   526,  8283,   491,   278,  7156, 29889,
           450,  1559, 29915, 29879,  5073,   653,  2602,   373,   278,  2625,
           310,   278,  6520,  7344, 29879, 12469,  4972,   322,   947,   451,
           766,  6685,   738,   373, 17696, 10298,   297,   278, 19500,  9088,
         29889,   450, 18830,  5177,  1122, 13100, 29892,   541,  1728,  5684,
          2472,   470,  2702, 29879,  4944, 29892,   372,   338, 12023,   393,
           278,  4038,  2820,   278, 14089,   287,  1559,   338, 15662,   310,
           263,  4943,  6520,  2975, 10483, 29889,    13,  2277, 29937,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  2277, 29937,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  2277, 29937,  7900, 22137, 29901,  3824, 29892,
           278,   321,  1484, 29899,  4287, 12402,  1078,   263,  2821,  4272,
         11952, 29892,  7344,   292,   263, 13747,  6210,  1550,  7952,   292,
           297,   967,   301,  1662, 29892, 23941,  4226,  4272, 19500,  5855,
         29889, 29871,    13,    13,  9190, 29892,   278,  7156,  3732,   263,
           301,  1662,  1735,   304,   278,  2175, 29889,   910,  3158,   338,
          5517,  2861,   304,   263,  1559,  4153, 14432,   297,   278,  1857,
           301,  1662, 29889,   450,   767, 12932,   369,   304,   278,  2175,
           301,  1662,   338, 23511,  8283,   408,   278,   301,  1662,   338,
          8967,   304,   367,  2821, 29892,  5662,  3864,   278,  2145,   950,
         10597,  6728,   310,   278,   321,  1484, 29899,  4287,  1623,   278,
         11952, 29889,    13,    13, 11760, 29892,   278,   321,  1484, 29899,
          4287, 13501,   385, 17686,   988,   278, 12469,  3578, 12169,  2654,
         29889,   512,  2933,   304,   445,  1735, 29892,   278,  7156,  5544,
         14981, 23522,   278,  1559,   304,   263,  4866,  5040, 29892,   752,
          5890,   411, 12469,  6865,   322,  1072,  8250, 29889,   450,  2654,
          3578, 14661,   727,  1122,   367,  4891, 12469,   470,  8939,   342,
           374,   550,   393,   278,  7156,  4225,   304,  7709,   304, 29889,
            13,    13, 12881,   635, 29892,   278,  1559,  9242,  5073,   653,
           472,   278, 17686,   408,   278, 12469,  3578, 18172,   304,  1510,
           278,  2654,  7182, 29889,   910,  3785,   310,   297, 10072,  9432,
         29879,   278,  7156, 29915, 29879,   594,  2276,   663,   304, 12469,
         14243, 29892,   408,  8401,   472,   263,  2654,  3578,  1033,  1121,
           297,  1035, 16719,   470,  6584,  1997,   583, 29889,  7133,   445,
           931, 29892,   278,  7156,  1795,   367,  8454,   363,   738,  3620,
           297,   278, 12469,  3578,   470, 20810, 18647,   322,  8939,   342,
           374,   550, 10223,   292,   363,  1009,  2446,  4337, 29889,    13,
          2277, 29937],
        [ -100,  -100,  -100,  -100,  2277, 29937,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  2277, 29937,  7900, 22137, 29901, 17044,
           449,   278,  6790, 14385, 29892,   278,   321,  1484, 29899,  4287,
          9242, 14089,   287,   373,   278,  2625,   310,   278,  6520, 29889,
          1670,   338,   694,  2702, 14881,   411,   916, 24413,   470,  8939,
           342,   374,   550,  2645,   445,  9088, 29892,   408,   278,  1559,
           338,  5073,   653,   363,   278,  4152,  1017,   310,   278,  4944,
           931,  3785, 29889,   450,  7156,   310,   278,   321,  1484, 29899,
          4287,  1122,   505, 14089,   287,   304,  1065,   385,  4589,   392,
           470,  3763, 10534,   297,   278,  1559, 29889,   910, 10483, 14088,
           263,  1209,   573,  4464,   310, 19500, 29892,   694,   916,  8820,
           470,   767, 12932,   874,   526,  8283,   491,   278,  7156, 29889,
           450,  1559, 29915, 29879,  5073,   653,  2602,   373,   278,  2625,
           310,   278,  6520,  7344, 29879, 12469,  4972,   322,   947,   451,
           766,  6685,   738,   373, 17696, 10298,   297,   278, 19500,  9088,
         29889,   450, 18830,  5177,  1122, 13100, 29892,   541,  1728,  5684,
          2472,   470,  2702, 29879,  4944, 29892,   372,   338, 12023,   393,
           278,  4038,  2820,   278, 14089,   287,  1559,   338, 15662,   310,
           263,  4943,  6520,  2975, 10483, 29889,    13,  2277, 29937,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100]], device='cuda:0'), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True,  True,  True,  True,  True,  True,  True, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False, False, False, False, False, False, False, False, False,
         False, False]], device='cuda:0'), 'images': tensor([[[[[-0.3178, -1.1937, -1.7339,  ..., -1.6025, -1.6171, -1.5587],
           [-0.2740, -1.1353, -1.6755,  ..., -1.6025, -1.5879, -1.5587],
           [-0.2010, -1.0477, -1.6171,  ..., -1.6025, -1.5879, -1.5733],
           ...,
           [-1.1937, -1.1937, -1.1937,  ..., -1.5149, -1.5149, -1.5295],
           [-1.2083, -1.1937, -1.1937,  ..., -1.5149, -1.5149, -1.5149],
           [-1.1937, -1.2083, -1.1937,  ..., -1.5295, -1.5149, -1.5149]],
          [[-1.1207, -1.1353, -1.1499,  ..., -1.6463, -1.6317, -1.6025],
           [-1.0769, -1.0769, -1.1061,  ..., -1.6463, -1.6317, -1.6025],
           [-0.9748, -0.9893, -1.0331,  ..., -1.6463, -1.6317, -1.6171],
           ...,
           [-1.2959, -1.2959, -1.2813,  ..., -1.5733, -1.5587, -1.5295],
           [-1.2813, -1.2667, -1.2667,  ..., -1.5733, -1.5587, -1.5441],
           [-1.2521, -1.2375, -1.2521,  ..., -1.5587, -1.5733, -1.5733]],
          [[-1.7339, -1.7339, -1.7193,  ..., -1.6317, -1.6317, -1.6171],
           [-1.7339, -1.7339, -1.7339,  ..., -1.6317, -1.6317, -1.6171],
           [-1.7193, -1.7193, -1.7047,  ..., -1.6171, -1.6171, -1.6317],
           ...,
           [-1.2083, -1.1645, -1.1499,  ..., -1.3543, -1.3835, -1.3689],
           [-1.2083, -1.1791, -1.1645,  ..., -1.3835, -1.4127, -1.3981],
           [-1.2229, -1.1937, -1.1791,  ..., -1.4127, -1.4127, -1.4127]],
          ...,
          [[-1.2959, -1.2521, -1.2229,  ..., -1.6609, -1.6609, -1.6317],
           [-1.2959, -1.2521, -1.2229,  ..., -1.6609, -1.6609, -1.6463],
           [-1.3105, -1.2521, -1.2375,  ..., -1.6463, -1.6463, -1.6317],
           ...,
           [-0.7704, -0.7704, -0.7558,  ..., -0.7266, -0.7412, -0.7850],
           [-0.9456, -0.9456, -0.9456,  ..., -0.7412, -0.7412, -0.7850],
           [-1.0185, -1.0331, -1.0185,  ..., -0.7412, -0.7412, -0.7850]],
          [[-1.2229, -1.2229, -1.1791,  ..., -1.6171, -1.6317, -1.6463],
           [-1.2667, -1.2521, -1.1645,  ..., -1.6025, -1.6317, -1.6463],
           [-1.2959, -1.2813, -1.1353,  ..., -1.5879, -1.6171, -1.6463],
           ...,
           [-0.5222, -0.4930, -0.4492,  ..., -0.7996, -0.7996, -0.8142],
           [-0.5660, -0.5222, -0.4638,  ..., -0.8142, -0.8288, -0.8288],
           [-0.5806, -0.5222, -0.4492,  ..., -0.8288, -0.8434, -0.8142]],
          [[-1.2375, -1.2083, -1.1791,  ..., -1.6317, -1.6463, -1.6463],
           [-1.2375, -1.2375, -1.1645,  ..., -1.6317, -1.6463, -1.6463],
           [-1.2521, -1.2375, -1.1353,  ..., -1.6317, -1.6317, -1.6317],
           ...,
           [-0.4638, -0.4200, -0.4200,  ..., -0.8288, -0.8142, -0.7704],
           [-0.5222, -0.4784, -0.4200,  ..., -0.7996, -0.7996, -0.7704],
           [-0.5514, -0.5076, -0.4346,  ..., -0.7704, -0.7704, -0.7850]]],
         [[[ 0.0488, -1.0017, -1.6320,  ..., -1.6921, -1.7071, -1.6771],
           [ 0.1089, -0.9417, -1.6020,  ..., -1.6771, -1.6771, -1.6771],
           [ 0.1839, -0.8816, -1.5570,  ..., -1.6771, -1.6771, -1.6921],
           ...,
           [-1.1368, -1.1368, -1.1368,  ..., -1.6020, -1.6020, -1.6170],
           [-1.1518, -1.1368, -1.1368,  ..., -1.6020, -1.6020, -1.6020],
           [-1.1368, -1.1518, -1.1518,  ..., -1.6170, -1.6170, -1.6020]],
          [[-1.4069, -1.4219, -1.4519,  ..., -1.6921, -1.6921, -1.6921],
           [-1.3919, -1.4069, -1.4369,  ..., -1.6771, -1.6771, -1.6921],
           [-1.3469, -1.3769, -1.3919,  ..., -1.6921, -1.6921, -1.6771],
           ...,
           [-1.4069, -1.4069, -1.3919,  ..., -1.6170, -1.6020, -1.5870],
           [-1.3769, -1.3769, -1.3919,  ..., -1.6020, -1.6170, -1.6170],
           [-1.3469, -1.3619, -1.3769,  ..., -1.6020, -1.6170, -1.6470]],
          [[-1.7071, -1.7071, -1.6921,  ..., -1.6771, -1.6921, -1.7071],
           [-1.7071, -1.7071, -1.6921,  ..., -1.6771, -1.6921, -1.7071],
           [-1.6921, -1.6921, -1.6921,  ..., -1.6621, -1.6771, -1.7071],
           ...,
           [-1.2869, -1.2568, -1.2418,  ..., -1.5270, -1.5420, -1.5270],
           [-1.2869, -1.2718, -1.2568,  ..., -1.5120, -1.5270, -1.5270],
           [-1.3019, -1.2718, -1.2568,  ..., -1.4970, -1.4970, -1.4970]],
          ...,
          [[-1.4519, -1.4369, -1.4219,  ..., -1.6771, -1.6921, -1.6771],
           [-1.4820, -1.4369, -1.4219,  ..., -1.6621, -1.6771, -1.6921],
           [-1.4970, -1.4519, -1.4369,  ..., -1.6470, -1.6771, -1.6921],
           ...,
           [-0.9267, -0.9267, -0.9117,  ..., -1.0467, -1.0317, -1.0167],
           [-1.0918, -1.1068, -1.0918,  ..., -1.0617, -1.0317, -1.0317],
           [-1.1668, -1.1818, -1.1668,  ..., -1.0467, -1.0317, -1.0467]],
          [[-1.4219, -1.4669, -1.4369,  ..., -1.6621, -1.6771, -1.6921],
           [-1.4369, -1.4970, -1.4219,  ..., -1.6320, -1.6621, -1.6921],
           [-1.4669, -1.5120, -1.3919,  ..., -1.6170, -1.6470, -1.6921],
           ...,
           [-0.6865, -0.6415, -0.6115,  ..., -1.0467, -1.0467, -1.0617],
           [-0.7166, -0.6715, -0.6115,  ..., -1.0617, -1.0767, -1.0767],
           [-0.7166, -0.6565, -0.5965,  ..., -1.0767, -1.0918, -1.0617]],
          [[-1.4219, -1.4369, -1.4069,  ..., -1.6771, -1.6921, -1.6921],
           [-1.4519, -1.4669, -1.4069,  ..., -1.6771, -1.6921, -1.6771],
           [-1.4669, -1.4970, -1.3919,  ..., -1.6771, -1.6921, -1.6921],
           ...,
           [-0.6115, -0.5665, -0.5665,  ..., -1.0767, -1.0617, -1.0767],
           [-0.6865, -0.6265, -0.5665,  ..., -1.0767, -1.0767, -1.0617],
           [-0.7166, -0.6565, -0.5815,  ..., -1.0767, -1.0767, -1.0617]]],
         [[[-0.9399, -1.2243, -1.4518,  ..., -1.4660, -1.4660, -1.4233],
           [-0.9256, -1.1816, -1.4233,  ..., -1.4518, -1.4518, -1.4233],
           [-0.9256, -1.1389, -1.4091,  ..., -1.4518, -1.4376, -1.4376],
           ...,
           [-1.0394, -1.0394, -1.0394,  ..., -1.4376, -1.4376, -1.4518],
           [-1.0536, -1.0394, -1.0394,  ..., -1.4376, -1.4376, -1.4376],
           [-1.0394, -1.0536, -1.0536,  ..., -1.4518, -1.4518, -1.4518]],
          [[-1.2527, -1.3096, -1.3522,  ..., -1.4091, -1.4233, -1.4376],
           [-1.2527, -1.2954, -1.3380,  ..., -1.4091, -1.4233, -1.4233],
           [-1.2243, -1.2669, -1.3096,  ..., -1.4233, -1.4233, -1.4376],
           ...,
           [-1.2527, -1.2669, -1.2669,  ..., -1.4091, -1.4233, -1.4376],
           [-1.2243, -1.2527, -1.2669,  ..., -1.4091, -1.4233, -1.4233],
           [-1.2100, -1.2385, -1.2527,  ..., -1.4091, -1.4233, -1.4376]],
          [[-1.4376, -1.4660, -1.4660,  ..., -1.4660, -1.4802, -1.4660],
           [-1.4660, -1.4660, -1.4660,  ..., -1.4660, -1.4660, -1.4802],
           [-1.4802, -1.4660, -1.4660,  ..., -1.4518, -1.4660, -1.4802],
           ...,
           [-1.1674, -1.1532, -1.1389,  ..., -1.4233, -1.4518, -1.4376],
           [-1.1958, -1.1674, -1.1532,  ..., -1.4376, -1.4376, -1.4376],
           [-1.2100, -1.1674, -1.1532,  ..., -1.4376, -1.4233, -1.4233]],
          ...,
          [[-1.3665, -1.3807, -1.3665,  ..., -1.4518, -1.4660, -1.4802],
           [-1.3522, -1.3665, -1.3522,  ..., -1.4518, -1.4518, -1.4660],
           [-1.3380, -1.3665, -1.3522,  ..., -1.4376, -1.4376, -1.4376],
           ...,
           [-0.9114, -0.9256, -0.8972,  ..., -1.2385, -1.2243, -1.2100],
           [-1.0963, -1.1105, -1.0963,  ..., -1.2527, -1.2243, -1.1816],
           [-1.1958, -1.1958, -1.1958,  ..., -1.2527, -1.2100, -1.1674]],
          [[-1.3238, -1.3380, -1.2954,  ..., -1.4233, -1.4376, -1.4802],
           [-1.3380, -1.3949, -1.3096,  ..., -1.4091, -1.4376, -1.4802],
           [-1.3665, -1.4518, -1.3522,  ..., -1.4091, -1.4376, -1.4802],
           ...,
           [-0.6839, -0.6270, -0.5986,  ..., -1.2385, -1.2669, -1.3096],
           [-0.7123, -0.6555, -0.5986,  ..., -1.2811, -1.3096, -1.3238],
           [-0.7123, -0.6555, -0.5844,  ..., -1.3238, -1.3380, -1.3096]],
          [[-1.3522, -1.4091, -1.3949,  ..., -1.4376, -1.4518, -1.4518],
           [-1.3522, -1.4091, -1.3522,  ..., -1.4376, -1.4518, -1.4376],
           [-1.3380, -1.3949, -1.3238,  ..., -1.4376, -1.4376, -1.4518],
           ...,
           [-0.5986, -0.5559, -0.5559,  ..., -1.3096, -1.2954, -1.2527],
           [-0.6697, -0.6128, -0.5701,  ..., -1.3238, -1.2954, -1.2527],
           [-0.6981, -0.6412, -0.5701,  ..., -1.3096, -1.2954, -1.2527]]]],
        [[[[-0.7996, -0.7996, -0.8872,  ..., -0.9602, -0.9164, -0.8580],
           [-0.8288, -0.8434, -0.9164,  ..., -0.9456, -0.9018, -0.7850],
           [-0.8580, -0.8872, -0.9748,  ..., -0.9018, -0.8434, -0.7120],
           ...,
           [-0.7704, -0.7558, -0.7120,  ..., -1.4419, -1.4565, -1.4565],
           [-0.7704, -0.7558, -0.7120,  ..., -1.4273, -1.4419, -1.4419],
           [-0.7412, -0.7412, -0.6974,  ..., -1.4273, -1.4419, -1.4273]],
          [[-0.8726, -0.8872, -0.9164,  ..., -0.9748, -0.9164, -0.8872],
           [-0.8872, -0.9018, -0.9456,  ..., -0.9310, -0.9164, -0.8142],
           [-0.8872, -0.9018, -0.9748,  ..., -0.8580, -0.8434, -0.6974],
           ...,
           [-0.7412, -0.7558, -0.7120,  ..., -1.4127, -1.4273, -1.4273],
           [-0.7412, -0.7266, -0.6974,  ..., -1.4127, -1.4127, -1.4127],
           [-0.7412, -0.7120, -0.6828,  ..., -1.4127, -1.4273, -1.4127]],
          [[-0.8580, -0.8434, -0.8872,  ..., -0.9893, -0.9310, -0.9164],
           [-0.8434, -0.8726, -0.9310,  ..., -0.9602, -0.9164, -0.8580],
           [-0.8580, -0.9018, -0.9893,  ..., -0.8726, -0.8142, -0.7266],
           ...,
           [-0.7266, -0.7266, -0.6974,  ..., -1.4273, -1.4419, -1.4419],
           [-0.7412, -0.7120, -0.6974,  ..., -1.4273, -1.4419, -1.4565],
           [-0.7412, -0.7120, -0.6974,  ..., -1.4273, -1.4565, -1.4711]],
          ...,
          [[-0.8434, -0.8872, -0.9164,  ..., -0.9748, -0.9456, -0.8580],
           [-0.8580, -0.8872, -0.9602,  ..., -0.9164, -0.9164, -0.8142],
           [-0.8872, -0.9164, -1.0185,  ..., -0.8288, -0.8288, -0.7558],
           ...,
           [-0.7120, -0.7120, -0.6974,  ..., -1.4565, -1.4711, -1.5003],
           [-0.7120, -0.6974, -0.6682,  ..., -1.4711, -1.4711, -1.4565],
           [-0.7120, -0.6828, -0.6536,  ..., -1.4711, -1.4565, -1.4419]],
          [[-0.8580, -0.8434, -0.8872,  ..., -0.9748, -0.9164, -0.8872],
           [-0.8288, -0.8580, -0.8872,  ..., -0.9602, -0.9018, -0.8288],
           [-0.8434, -0.8872, -0.9164,  ..., -0.8872, -0.8142, -0.7266],
           ...,
           [-1.2229, -1.2083, -1.2229,  ..., -1.4419, -1.4419, -1.4419],
           [-1.2229, -1.2083, -1.2229,  ..., -1.4273, -1.4273, -1.4273],
           [-1.2229, -1.1791, -1.2083,  ..., -1.3981, -1.4273, -1.4419]],
          [[-0.9018, -0.8872, -0.9018,  ..., -0.9893, -0.9164, -0.9018],
           [-0.8872, -0.8872, -0.9164,  ..., -0.9456, -0.9018, -0.8580],
           [-0.8726, -0.8872, -0.9456,  ..., -0.8872, -0.8434, -0.7558],
           ...,
           [-0.8434, -0.7558, -0.6682,  ..., -1.4127, -1.4273, -1.4565],
           [-0.8288, -0.7412, -0.6536,  ..., -1.4127, -1.4273, -1.4419],
           [-0.8288, -0.7266, -0.6682,  ..., -1.4127, -1.4419, -1.4565]]],
         [[[-1.2869, -1.3019, -1.3469,  ..., -1.0767, -1.0167, -0.9567],
           [-1.3169, -1.3319, -1.3769,  ..., -1.0467, -0.9867, -0.8967],
           [-1.3319, -1.3469, -1.3919,  ..., -0.9867, -0.9267, -0.8066],
           ...,
           [-1.2268, -1.2118, -1.1968,  ..., -1.3469, -1.3919, -1.3919],
           [-1.2268, -1.2118, -1.1818,  ..., -1.3319, -1.3919, -1.3769],
           [-1.1968, -1.1968, -1.1818,  ..., -1.3169, -1.3769, -1.3619]],
          [[-1.2718, -1.2869, -1.3469,  ..., -1.0617, -0.9717, -0.9417],
           [-1.2869, -1.2869, -1.3619,  ..., -1.0317, -0.9867, -0.8816],
           [-1.3019, -1.3019, -1.3769,  ..., -0.9867, -0.9417, -0.8066],
           ...,
           [-1.2418, -1.2568, -1.2268,  ..., -1.3769, -1.3919, -1.3919],
           [-1.2418, -1.2118, -1.1968,  ..., -1.3769, -1.3769, -1.3769],
           [-1.2418, -1.1968, -1.1818,  ..., -1.3769, -1.3919, -1.3769]],
          [[-1.3019, -1.2869, -1.3019,  ..., -1.0918, -1.0317, -1.0017],
           [-1.2869, -1.3169, -1.3469,  ..., -1.0467, -1.0017, -0.9417],
           [-1.3019, -1.3469, -1.4069,  ..., -0.9717, -0.9117, -0.8216],
           ...,
           [-1.2268, -1.2118, -1.1818,  ..., -1.3469, -1.4069, -1.4219],
           [-1.2418, -1.2118, -1.1818,  ..., -1.3469, -1.4069, -1.4069],
           [-1.2418, -1.1968, -1.1818,  ..., -1.3319, -1.3919, -1.3919]],
          ...,
          [[-1.2869, -1.3319, -1.3619,  ..., -1.1068, -1.0617, -0.9717],
           [-1.2869, -1.3169, -1.3769,  ..., -1.0467, -1.0017, -0.9117],
           [-1.2718, -1.3169, -1.4069,  ..., -0.9567, -0.9117, -0.8366],
           ...,
           [-1.2418, -1.2568, -1.2118,  ..., -1.3319, -1.3919, -1.4069],
           [-1.2418, -1.2268, -1.1968,  ..., -1.3319, -1.3769, -1.3619],
           [-1.2418, -1.1968, -1.1818,  ..., -1.3319, -1.3769, -1.3469]],
          [[-1.3019, -1.2869, -1.3319,  ..., -1.0617, -1.0167, -0.9717],
           [-1.2869, -1.2869, -1.3319,  ..., -1.0317, -0.9867, -0.9117],
           [-1.2718, -1.3019, -1.3619,  ..., -0.9567, -0.9117, -0.8066],
           ...,
           [-1.3619, -1.3469, -1.3469,  ..., -1.3469, -1.3769, -1.3769],
           [-1.3469, -1.3169, -1.3469,  ..., -1.3319, -1.3619, -1.3619],
           [-1.3469, -1.2869, -1.3319,  ..., -1.3019, -1.3619, -1.3769]],
          [[-1.3019, -1.2869, -1.3019,  ..., -1.0918, -1.0167, -0.9867],
           [-1.2869, -1.2869, -1.3319,  ..., -1.0467, -1.0017, -0.9417],
           [-1.2718, -1.2869, -1.3919,  ..., -0.9717, -0.9267, -0.8366],
           ...,
           [-1.2869, -1.1818, -1.1368,  ..., -1.3169, -1.3769, -1.3919],
           [-1.2869, -1.1818, -1.1368,  ..., -1.3019, -1.3619, -1.3769],
           [-1.2718, -1.1668, -1.1368,  ..., -1.3019, -1.3469, -1.3469]]],
         [[[-1.3096, -1.3238, -1.2811,  ..., -0.9825, -0.9541, -0.9114],
           [-1.2811, -1.2954, -1.2811,  ..., -0.9683, -0.9541, -0.8688],
           [-1.2527, -1.2669, -1.2811,  ..., -0.9541, -0.9114, -0.7977],
           ...,
           [-1.2243, -1.2243, -1.1958,  ..., -1.1958, -1.2385, -1.2385],
           [-1.2243, -1.2243, -1.1816,  ..., -1.1958, -1.2243, -1.2243],
           [-1.2100, -1.2100, -1.1816,  ..., -1.1816, -1.2243, -1.2100]],
          [[-1.2385, -1.2527, -1.3096,  ..., -1.0394, -0.9256, -0.9114],
           [-1.2527, -1.2669, -1.3096,  ..., -1.0252, -0.9683, -0.8688],
           [-1.2669, -1.2669, -1.3238,  ..., -0.9967, -0.9541, -0.8261],
           ...,
           [-1.2385, -1.2385, -1.2385,  ..., -1.1958, -1.2243, -1.2243],
           [-1.2385, -1.2243, -1.2243,  ..., -1.1958, -1.2243, -1.2100],
           [-1.2385, -1.2100, -1.2100,  ..., -1.1958, -1.2243, -1.2100]],
          [[-1.2527, -1.2527, -1.2385,  ..., -1.0678, -1.0110, -0.9967],
           [-1.2243, -1.2385, -1.2811,  ..., -1.0394, -0.9967, -0.9399],
           [-1.2243, -1.2527, -1.3238,  ..., -0.9967, -0.9399, -0.8403],
           ...,
           [-1.2385, -1.2385, -1.1958,  ..., -1.1958, -1.2385, -1.2527],
           [-1.2243, -1.2100, -1.1816,  ..., -1.1958, -1.2385, -1.2527],
           [-1.2385, -1.2100, -1.1816,  ..., -1.1958, -1.2385, -1.2527]],
          ...,
          [[-1.2527, -1.2669, -1.3096,  ..., -1.0536, -0.9399, -0.8545],
           [-1.2243, -1.2527, -1.3238,  ..., -0.9967, -0.9256, -0.8403],
           [-1.2100, -1.2385, -1.3380,  ..., -0.9399, -0.8830, -0.8119],
           ...,
           [-1.2243, -1.2385, -1.1958,  ..., -1.1816, -1.2100, -1.2385],
           [-1.2243, -1.2100, -1.1816,  ..., -1.1958, -1.2100, -1.1958],
           [-1.2243, -1.1816, -1.1674,  ..., -1.1958, -1.1958, -1.1674]],
          [[-1.2527, -1.2385, -1.2669,  ..., -1.0536, -0.9967, -0.9541],
           [-1.2243, -1.2243, -1.2811,  ..., -1.0252, -0.9683, -0.8972],
           [-1.1958, -1.2243, -1.2954,  ..., -0.9541, -0.8830, -0.7977],
           ...,
           [-1.2243, -1.2243, -1.2243,  ..., -1.1958, -1.2243, -1.2243],
           [-1.2243, -1.1958, -1.2243,  ..., -1.1816, -1.2100, -1.2100],
           [-1.2243, -1.1674, -1.2100,  ..., -1.1674, -1.2100, -1.2243]],
          [[-1.3096, -1.3096, -1.2811,  ..., -1.0536, -0.9541, -0.9399],
           [-1.2669, -1.2669, -1.3096,  ..., -1.0252, -0.9541, -0.9256],
           [-1.2385, -1.2385, -1.3238,  ..., -0.9825, -0.9114, -0.8403],
           ...,
           [-1.2385, -1.1389, -1.1247,  ..., -1.1674, -1.2243, -1.2385],
           [-1.2385, -1.1389, -1.1389,  ..., -1.1674, -1.2100, -1.2243],
           [-1.2527, -1.1532, -1.1674,  ..., -1.1674, -1.2100, -1.2100]]]]],
       device='cuda:0'), 'conv_type': 'multi'}
tensor(1.8896, device='cuda:0')