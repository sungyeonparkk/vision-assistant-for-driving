| distributed init (rank 0, world 1): env://
Loading VIT
2023-09-12 20:29:50,694 [INFO]
=====  Running Parameters    =====
2023-09-12 20:29:50,694 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 2,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 3e-05,
    "iters_per_epoch": 5,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 100,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "/output/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "video_text_pretrain",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 5,
    "weight_decay": 0.05,
    "world_size": 1
}
2023-09-12 20:29:50,695 [INFO]
======  Dataset Attributes  ======
2023-09-12 20:29:50,695 [INFO]
======== bdd_instruct =======
2023-09-12 20:29:50,695 [INFO] {
    "build_info": {
        "train": {
            "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
            "videos_dir": "/root/BDD-X/"
        },
        "val": {
            "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
            "videos_dir": "/root/BDD-X/"
        }
    },
    "data_type": "video",
    "model_type": "llama_v2",
    "num_video_query_token": 32,
    "text_processor": {
        "train": {
            "name": "blip_caption"
        },
        "val": {
            "name": "blip_caption"
        }
    },
    "tokenizer_name": "/root/ckpt/llama-2-7b-chat-hf",
    "vis_processor": {
        "train": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        },
        "val": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        }
    }
}
2023-09-12 20:29:50,695 [INFO]
======  Model Attributes  ======
2023-09-12 20:29:50,696 [INFO] {
    "arch": "video_llama",
    "ckpt": "/root/ckpt/VL_LLaMA_2_7B_Finetuned.pth",
    "drop_path_rate": 0,
    "equip_audio_branch": false,
    "freeze_qformer": true,
    "freeze_vit": true,
    "frozen_audio_Qformer": true,
    "frozen_llama_proj": false,
    "frozen_video_Qformer": false,
    "fusion_head_layers": 2,
    "fusion_header_type": "seqTransf",
    "image_size": 224,
    "llama_model": "/root/ckpt/llama-2-7b-chat-hf",
    "max_frame_pos": 32,
    "max_txt_len": 320,
    "model_type": "pretrain_llama_v2",
    "num_query_token": 32,
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
2023-09-12 20:29:50,696 [INFO] Building datasets...
Loading VIT Done
Loading Q-Former
2023-09-12 20:30:30,396 [INFO] freeze vision encoder
2023-09-12 20:30:33,701 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth
2023-09-12 20:30:33,722 [INFO] freeze Qformer
2023-09-12 20:30:33,722 [INFO] Loading Q-Former Done
2023-09-12 20:30:33,722 [INFO] Loading LLAMA Tokenizer
Using pad_token, but it is not set yet.
2023-09-12 20:30:33,845 [INFO] Loading LLAMA Model

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.04s/it]
2023-09-12 20:31:41,571 [INFO] Loading LLAMA Done
2023-09-12 20:31:41,571 [INFO] Loading LLAMA proj
2023-09-12 20:31:41,593 [INFO] LLAMA proj is not frozen
2023-09-12 20:31:41,593 [INFO] Loading llama_proj Done
Load first Checkpoint: /root/ckpt/VL_LLaMA_2_7B_Finetuned.pth
2023-09-12 20:31:42,687 [INFO] video_Qformer is not frozen
2023-09-12 20:31:42,839 [INFO] Start training
module.video_query_tokens
module.llama_proj.weight
module.llama_proj.bias
module.video_frame_position_embedding.weight
module.video_Qformer.bert.embeddings.LayerNorm.weight
module.video_Qformer.bert.embeddings.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
2023-09-12 20:31:46,659 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-09-12 20:31:46,659 [INFO] Loaded 10 records for train split from the dataset.
2023-09-12 20:31:46,659 [INFO] Loaded 10 records for val split from the dataset.
2023-09-12 20:31:46,674 [INFO] number of trainable parameters: 22104064
2023-09-12 20:31:46,674 [INFO] Start training epoch 0, 5 iters per inner epoch.
Train: data epoch: [0]  [0/5]  eta: 0:00:12  lr: 0.000001  loss: 2.2720  time: 2.4568  data: 0.0000  max mem: 32999
2023-09-12 20:31:49,135 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [4/5]  eta: 0:00:00  lr: 0.000024  loss: 1.9258  time: 0.8300  data: 0.0000  max mem: 35718
Train: data epoch: [0] Total time: 0:00:04 (0.8303 s / it)
val_split : val
2023-09-12 20:31:50,827 [INFO] Averaged stats: lr: 0.0000  loss: 2.0897
2023-09-12 20:31:50,829 [INFO] Evaluating on val.
2023-09-12 20:31:50,842 [INFO] Start training
2023-09-12 20:31:50,861 [INFO] Start training epoch 1, 5 iters per inner epoch.
Train: data epoch: [1]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.9663  time: 3.2248  data: 0.0000  max mem: 35718
Train: data epoch: [1]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.9333  time: 0.9322  data: 0.0000  max mem: 35718
Train: data epoch: [1] Total time: 0:00:04 (0.9326 s / it)
val_split : val
2023-09-12 20:31:55,525 [INFO] Averaged stats: lr: 0.0000  loss: 1.8879
2023-09-12 20:31:55,527 [INFO] Evaluating on val.
2023-09-12 20:31:55,540 [INFO] Start training
2023-09-12 20:31:55,559 [INFO] Start training epoch 2, 5 iters per inner epoch.
Train: data epoch: [2]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.7643  time: 3.2550  data: 0.0000  max mem: 35718
2023-09-12 20:32:00,303 [INFO] Averaged stats: lr: 0.0000  loss: 1.7928
2023-09-12 20:32:00,308 [INFO] Evaluating on val.
2023-09-12 20:32:00,322 [INFO] Start training
2023-09-12 20:32:00,340 [INFO] Start training epoch 3, 5 iters per inner epoch.
Train: data epoch: [2]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.8292  time: 0.9483  data: 0.0000  max mem: 35718
Train: data epoch: [2] Total time: 0:00:04 (0.9487 s / it)
val_split : val
Train: data epoch: [3]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.6741  time: 3.2348  data: 0.0000  max mem: 35718
Train: data epoch: [3]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.6887  time: 0.9391  data: 0.0000  max mem: 35718
Train: data epoch: [3] Total time: 0:00:04 (0.9395 s / it)
val_split : val
2023-09-12 20:32:05,039 [INFO] Averaged stats: lr: 0.0000  loss: 1.7303
2023-09-12 20:32:05,041 [INFO] Evaluating on val.
2023-09-12 20:32:05,055 [INFO] Start training
2023-09-12 20:32:05,074 [INFO] Start training epoch 4, 5 iters per inner epoch.
Train: data epoch: [4]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.5905  time: 3.2409  data: 0.0000  max mem: 35718
Train: data epoch: [4]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.7931  time: 0.9390  data: 0.0000  max mem: 35718
Train: data epoch: [4] Total time: 0:00:04 (0.9393 s / it)
val_split : val
2023-09-12 20:32:09,771 [INFO] Averaged stats: lr: 0.0000  loss: 1.6811
2023-09-12 20:32:09,773 [INFO] Evaluating on val.
2023-09-12 20:32:09,786 [INFO] Start training
2023-09-12 20:32:09,805 [INFO] Start training epoch 5, 5 iters per inner epoch.
Train: data epoch: [5]  [0/5]  eta: 0:00:15  lr: 0.000030  loss: 1.6205  time: 3.1702  data: 0.0000  max mem: 35718
2023-09-12 20:32:14,445 [INFO] Averaged stats: lr: 0.0000  loss: 1.6230
2023-09-12 20:32:14,447 [INFO] Evaluating on val.
2023-09-12 20:32:14,461 [INFO] Start training
2023-09-12 20:32:14,480 [INFO] Start training epoch 6, 5 iters per inner epoch.
Train: data epoch: [5]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.5089  time: 0.9275  data: 0.0000  max mem: 35718
Train: data epoch: [5] Total time: 0:00:04 (0.9278 s / it)
val_split : val
Train: data epoch: [6]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.5200  time: 3.2981  data: 0.0000  max mem: 35718
Train: data epoch: [6]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.4715  time: 0.9554  data: 0.0000  max mem: 35718
Train: data epoch: [6] Total time: 0:00:04 (0.9557 s / it)
val_split : val
2023-09-12 20:32:19,259 [INFO] Averaged stats: lr: 0.0000  loss: 1.5721
2023-09-12 20:32:19,262 [INFO] Evaluating on val.
2023-09-12 20:32:19,275 [INFO] Start training
2023-09-12 20:32:19,293 [INFO] Start training epoch 7, 5 iters per inner epoch.
Train: data epoch: [7]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.5845  time: 3.2990  data: 0.0000  max mem: 35718
Train: data epoch: [7]  [4/5]  eta: 0:00:00  lr: 0.000030  loss: 1.4863  time: 0.9383  data: 0.0000  max mem: 35718
Train: data epoch: [7] Total time: 0:00:04 (0.9386 s / it)
val_split : val
2023-09-12 20:32:23,987 [INFO] Averaged stats: lr: 0.0000  loss: 1.5338
2023-09-12 20:32:23,990 [INFO] Evaluating on val.
2023-09-12 20:32:24,002 [INFO] Start training
2023-09-12 20:32:24,021 [INFO] Start training epoch 8, 5 iters per inner epoch.
Train: data epoch: [8]  [0/5]  eta: 0:00:16  lr: 0.000030  loss: 1.6832  time: 3.2663  data: 0.0000  max mem: 35718
Error in sys.excepthook:
Traceback (most recent call last):
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/lib/exit_hooks.py", line 52, in exc_handler
    traceback.print_exception(exc_type, exc, tb)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/traceback.py", line 103, in print_exception
    for line in TracebackException(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/exceptiongroup/_formatting.py", line 96, in __init__
    self.stack = traceback.StackSummary.extract(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/traceback.py", line 359, in extract
    result.append(FrameSummary(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/traceback.py", line 260, in __init__
    self.locals = {k: repr(v) for k, v in locals.items()} if locals else None
KeyboardInterrupt
Original exception was:
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/train.py", line 114, in <module>
    main()
  File "/root/vision-assistant-for-driving/train.py", line 110, in main
    runner.train(wandb)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 378, in train
    train_stats = self.train_epoch(cur_epoch)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 445, in train_epoch
    return self.task.train_epoch(
  File "/root/vision-assistant-for-driving/video_llama/tasks/base_task.py", line 118, in train_epoch
    return self._train_inner_loop(
  File "/root/vision-assistant-for-driving/video_llama/tasks/base_task.py", line 223, in _train_inner_loop
    loss = self.train_step(model=model, samples=samples)
  File "/root/vision-assistant-for-driving/video_llama/tasks/base_task.py", line 71, in train_step
    loss = model(samples)["loss"]
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/models/video_llama.py", line 470, in forward
    outputs = self.llama_model(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/models/modeling_llama.py", line 676, in forward
    outputs = self.model(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/models/modeling_llama.py", line 565, in forward
    layer_outputs = decoder_layer(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/models/modeling_llama.py", line 275, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/models/modeling_llama.py", line 180, in forward
    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt