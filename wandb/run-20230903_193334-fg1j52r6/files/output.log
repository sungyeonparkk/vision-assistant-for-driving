| distributed init (rank 0, world 1): env://
/root/ckpt/llama-2-7b-chat-hf
Loading VIT
2023-09-03 19:33:40,449 [INFO]
=====  Running Parameters    =====
2023-09-03 19:33:40,450 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 2,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 3e-05,
    "iters_per_epoch": 1000,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 3,
    "min_lr": 1e-05,
    "num_workers": 4,
    "output_dir": "/output/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "video_text_pretrain",
    "train_splits": [
        "train"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 1000,
    "weight_decay": 0.05,
    "world_size": 1
}
2023-09-03 19:33:40,450 [INFO]
======  Dataset Attributes  ======
2023-09-03 19:33:40,451 [INFO]
======== bdd_instruct =======
2023-09-03 19:33:40,451 [INFO] {
    "build_info": {
        "anno_dir": "/root/vision-assistant-for-driving/data/BDD_train_data/BDD-Instruct-10.json",
        "videos_dir": "/root/BDD-X/"
    },
    "data_type": "video",
    "model_type": "llama_v2",
    "num_video_query_token": 32,
    "text_processor": {
        "train": {
            "name": "blip_caption"
        }
    },
    "tokenizer_name": "/root/ckpt/llama-2-7b-chat-hf",
    "vis_processor": {
        "train": {
            "image_size": 224,
            "n_frms": 8,
            "name": "alpro_video_train"
        }
    }
}
2023-09-03 19:33:40,451 [INFO]
======  Model Attributes  ======
2023-09-03 19:33:40,451 [INFO] {
    "arch": "video_llama",
    "ckpt": "/root/ckpt/VL_LLaMA_2_7B_Finetuned.pth",
    "drop_path_rate": 0,
    "equip_audio_branch": false,
    "freeze_qformer": true,
    "freeze_vit": true,
    "frozen_audio_Qformer": true,
    "frozen_llama_proj": false,
    "frozen_video_Qformer": false,
    "fusion_head_layers": 2,
    "fusion_header_type": "seqTransf",
    "image_size": 224,
    "llama_model": "/root/ckpt/llama-2-7b-chat-hf",
    "max_frame_pos": 32,
    "max_txt_len": 320,
    "model_type": "pretrain_llama_v2",
    "num_query_token": 32,
    "prompt": "",
    "use_grad_checkpoint": false,
    "vit_precision": "fp16"
}
2023-09-03 19:33:40,452 [INFO] Building datasets...
Loading VIT Done
Loading Q-Former
2023-09-03 19:34:18,051 [INFO] freeze vision encoder
2023-09-03 19:34:24,979 [INFO] load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained_flant5xxl.pth
2023-09-03 19:34:24,996 [INFO] freeze Qformer
2023-09-03 19:34:24,996 [INFO] Loading Q-Former Done
2023-09-03 19:34:24,996 [INFO] Loading LLAMA Tokenizer
Using pad_token, but it is not set yet.
2023-09-03 19:34:25,084 [INFO] Loading LLAMA Model

Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.46s/it]
2023-09-03 19:35:37,691 [INFO] Loading LLAMA Done
2023-09-03 19:35:37,691 [INFO] Loading LLAMA proj
2023-09-03 19:35:37,707 [INFO] LLAMA proj is not frozen
2023-09-03 19:35:37,707 [INFO] Loading llama_proj Done
Load first Checkpoint: /root/ckpt/VL_LLaMA_2_7B_Finetuned.pth
2023-09-03 19:35:38,733 [INFO] video_Qformer is not frozen
2023-09-03 19:35:39,702 [INFO] Start training
module.video_query_tokens
module.llama_proj.weight
module.llama_proj.bias
module.video_frame_position_embedding.weight
module.video_Qformer.bert.embeddings.LayerNorm.weight
module.video_Qformer.bert.embeddings.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.0.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.0.output_query.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.attention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.attention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.query.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.key.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.self.value.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.dense.bias
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.crossattention.output.LayerNorm.bias
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.intermediate_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.dense.weight
module.video_Qformer.bert.encoder.layer.1.output_query.dense.bias
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.weight
module.video_Qformer.bert.encoder.layer.1.output_query.LayerNorm.bias
2023-09-03 19:35:43,670 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2023-09-03 19:35:43,670 [INFO] Loaded 10 records for train split from the dataset.
2023-09-03 19:35:43,685 [INFO] number of trainable parameters: 22104064
2023-09-03 19:35:43,686 [INFO] Start training epoch 0, 1000 iters per inner epoch.
Train: data epoch: [0]  [   0/1000]  eta: 0:43:48  lr: 0.000001  loss: 2.2720  time: 2.6283  data: 0.0000  max mem: 32998
2023-09-03 19:35:46,317 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [  50/1000]  eta: 0:15:18  lr: 0.000002  loss: 1.9811  time: 0.9457  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 100/1000]  eta: 0:14:17  lr: 0.000004  loss: 1.7795  time: 0.9394  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 150/1000]  eta: 0:13:23  lr: 0.000005  loss: 1.7321  time: 0.9349  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 200/1000]  eta: 0:12:34  lr: 0.000007  loss: 1.4594  time: 0.9286  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 250/1000]  eta: 0:11:46  lr: 0.000008  loss: 1.1986  time: 0.9394  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 300/1000]  eta: 0:10:58  lr: 0.000010  loss: 1.1176  time: 0.9354  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 350/1000]  eta: 0:10:09  lr: 0.000011  loss: 0.7571  time: 0.9312  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 400/1000]  eta: 0:09:22  lr: 0.000013  loss: 0.5663  time: 0.9209  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 450/1000]  eta: 0:08:35  lr: 0.000014  loss: 0.4302  time: 0.9315  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 500/1000]  eta: 0:07:47  lr: 0.000016  loss: 0.3630  time: 0.9336  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 550/1000]  eta: 0:07:00  lr: 0.000017  loss: 0.2741  time: 0.9229  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 600/1000]  eta: 0:06:14  lr: 0.000018  loss: 0.1129  time: 0.9508  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 650/1000]  eta: 0:05:27  lr: 0.000020  loss: 0.1139  time: 0.9668  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 700/1000]  eta: 0:04:40  lr: 0.000021  loss: 0.0925  time: 0.9366  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 750/1000]  eta: 0:03:53  lr: 0.000023  loss: 0.0810  time: 0.9328  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 800/1000]  eta: 0:03:06  lr: 0.000024  loss: 0.0779  time: 0.9278  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 850/1000]  eta: 0:02:20  lr: 0.000026  loss: 0.0579  time: 0.9686  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 900/1000]  eta: 0:01:33  lr: 0.000027  loss: 0.0599  time: 0.9323  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 950/1000]  eta: 0:00:46  lr: 0.000029  loss: 0.0501  time: 0.9229  data: 0.0000  max mem: 35724
Train: data epoch: [0]  [ 999/1000]  eta: 0:00:00  lr: 0.000030  loss: 0.0512  time: 0.9378  data: 0.0000  max mem: 35724
Train: data epoch: [0] Total time: 0:15:32 (0.9329 s / it)
2023-09-03 19:51:16,609 [INFO] Averaged stats: lr: 0.0000  loss: 0.6696
2023-09-03 19:51:16,612 [INFO] No validation splits found.
2023-09-03 19:51:16,631 [INFO] Saving checkpoint at epoch 0 to /output/20230903193/checkpoint_0.pth.
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/train.py", line 114, in <module>
    main()
  File "/root/vision-assistant-for-driving/train.py", line 110, in main
    runner.train(wandb)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 407, in train
    self._save_checkpoint(cur_epoch, is_best=False, wandb=wandb)
  File "/root/vision-assistant-for-driving/video_llama/common/dist_utils.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 601, in _save_checkpoint
    wandb.save(save_obj)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 341, in wrapper_fn
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 331, in wrapper
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1784, in save
    return self._save(glob_str, base_path, policy)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1799, in _save
    raise ValueError("Must call wandb.save(glob_str) with glob_str a str")
ValueError: Must call wandb.save(glob_str) with glob_str a str
Traceback (most recent call last):
  File "/root/vision-assistant-for-driving/train.py", line 114, in <module>
    main()
  File "/root/vision-assistant-for-driving/train.py", line 110, in main
    runner.train(wandb)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 407, in train
    self._save_checkpoint(cur_epoch, is_best=False, wandb=wandb)
  File "/root/vision-assistant-for-driving/video_llama/common/dist_utils.py", line 112, in wrapper
    return func(*args, **kwargs)
  File "/root/vision-assistant-for-driving/video_llama/runners/runner_base.py", line 601, in _save_checkpoint
    wandb.save(save_obj)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 341, in wrapper_fn
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 331, in wrapper
    return func(self, *args, **kwargs)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1784, in save
    return self._save(glob_str, base_path, policy)
  File "/root/miniconda3/envs/videollama2/lib/python3.9/site-packages/wandb/sdk/wandb_run.py", line 1799, in _save
    raise ValueError("Must call wandb.save(glob_str) with glob_str a str")
ValueError: Must call wandb.save(glob_str) with glob_str a str